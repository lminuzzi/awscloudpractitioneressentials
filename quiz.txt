AWS Cloud Practitioner Essentials
https://www.aws.training/Details/eLearning?id=60697

MODULE 1
Summarize the benefits of AWS.
Describe differences between on-demand delivery and cloud deployments.
Summarize the pay-as-you-go pricing model.

=> Cloud-Based Deployment
Run all parts of the application in the cloud.
Migrate existing applications to the cloud.
Design and build new applications in the cloud.

In a cloud-based deployment model, you can migrate existing applications to the cloud, or you can design and build new applications in the cloud.
You can build those applications on low-level infrastructure that requires your IT staff to manage them.
Alternatively, you can build them using higher-level services that reduce the management, architecting, and scaling requirements of the core infrastructure.
For example, a company might create an application consisting of virtual servers, databases, and networking components that are fully based in the cloud.


=> On-Premises Deployment
Deploy resources by using virtualization and resource management tools.
Increase resource utilization by using application management and virtualization technologies.

On-premises deployment is also known as a private cloud deployment. In this model, 
resources are deployed on premises by using virtualization and resource management tools.
For example, you might have applications that run on technology that is fully kept in your on-premises data center.
Though this model is much like legacy IT infrastructure, its incorporation of application management and 
virtualization technologies helps to increase resource utilization.


=> Hybrid Deployment
Connect cloud-based resources to on-premises infrastructure.
Integrate cloud-based resources with legacy IT applications.

In a hybrid deployment, cloud-based resources are connected to on-premises infrastructure. You might want to use this approach in a number of situations.
For example, you have legacy applications that are better maintained on premises, or government regulations require your business to keep certain records on premises.
For example, suppose that a company wants to use cloud services that can automate batch data processing and analytics. 
However, the company has several legacy applications that are more suitable on premises and will not be migrated to the cloud.
With a hybrid deployment, the company would be able to keep the legacy applications on premises while 
benefiting from the data and analytics services that run in the cloud.


=> Benefits
- Trade upfront expense for variable expense
- Stop spending money to run and maintain data centers
- Stop guessing capacity
- Benefit from massive economies of scale
- Increase speed and agility
- Go global in minutes


=> QUIZ

1 - What is cloud computing?

a) Backing up files that are stored on desktop and mobile devices to prevent data loss

b) Deploying applications connected to on-premises infrastructure

c) Running code without needing to manage or provision servers

d) On-demand delivery of IT resources and applications through the internet with pay-as-you-go pricing

The correct response option is On-demand delivery of IT resources and applications through the internet with pay-as-you-go pricing.



The other response options are incorrect because:

It is possible to back up files to the cloud, but this response option does not describe cloud computing as a whole.
Deploying applications connected to on-premises infrastructure is a sample use case for a hybrid cloud deployment. Remember that cloud computing also has 
cloud and on-premises (or private cloud) deployment models.
AWS Lambda is an AWS service that lets you run code without needing to manage or provision servers.
This description does not describe cloud computing as a whole. AWS Lambda is explained in greater detail later in the course.


2 - What is another name for on-premises deployment?

a) Private cloud deployment

b) Cloud-based application

c) Hybrid deployment

d) AWS Cloud

The correct response option is Private cloud deployment.



The other response options are incorrect because:

Cloud-based applications are fully deployed in the cloud and do not have any parts that run on premises.
A hybrid deployment connects infrastructure and applications between cloud-based resources 
and existing resources that are not in the cloud, such as on-premises resources.
However, a hybrid deployment is not equivalent to an on-premises deployment because it involves resources that are located in the cloud.
The AWS Cloud offers three cloud deployment models: cloud, hybrid, and on-premises. 
This response option is incorrect because the AWS Cloud is not equivalent to only an on-premises deployment.


3 - How does the scale of cloud computing help you to save costs?

a) You do not have to invest in technology resources before using them.

b) The aggregated cloud usage from a large number of customers results in lower pay-as-you-go prices.

c) Accessing services on-demand helps to prevent excess or limited capacity.

d) You can quickly deploy applications to customers and provide them with low latency.

The correct response option is The aggregated cloud usage from a large number of customers results in lower pay-as-you-go prices.


This answer describes how customers can benefit from massive economies of scale in cloud computing.

The other response options are incorrect because:

Not having to invest in technology resources before using them relates to Trade upfront expense for variable expense.
Accessing services on-demand to prevent excess or limited capacity relates to Stop guessing capacity.
Quickly deploying applications to customers and providing them with low latency relates to Go global in minutes.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------

MODULE 2
Describe the benefits of Amazon EC2 at a basic level.
Identify the different Amazon EC2 instance types.
Differentiate between the various billing options for Amazon EC2.
Summarize the benefits of Amazon EC2 Auto Scaling.
Summarize the benefits of Elastic Load Balancing.
Give an example of the uses for Elastic Load Balancing.
Summarize the differences between Amazon Simple Notification Service (Amazon SNS) and Amazon Simple Queue Service (Amazon SQS).
Summarize additional AWS compute options.


=> Amazon Elastic Compute Cloud (Amazon EC2) provides secure, resizable compute capacity in the cloud as Amazon EC2 instances. 
Launch
First, you launch an instance. Begin by selecting a template with basic configurations for your instance. 
These configurations include the operating system, application server, or applications.
You also select the instance type, which is the specific hardware configuration of your instance.
As you are preparing to launch an instance, you specify security settings to control the network traffic that can flow into and out of your instance.
Later in this course, we will explore Amazon EC2 security features in greater detail.

Connect
Next, connect to the instance. You can connect to the instance in several ways. 
Your programs and applications have multiple different methods to connect directly to the instance and exchange data.
Users can also connect to the instance by logging in and accessing the computer desktop.

Use
After you have connected to the instance, you can begin using it. You can run commands to install software, add storage, copy and organize files, and more.

Multitenancy: sharing underlying hardware. The hypervisor is responsible for coordinating this multitenancy and it is managed by AWS. 
The hypervisor is responsible for isolating the virtual machines from each other as they share resources from the host. This means EC2 instances are secure. 
Even though they may be sharing resources, one EC2 instance is not aware of any other EC2 instances also on that host. They are secure and separate from each other. 

When you provision an EC2 instance, you can choose the operating system based on either Windows or Linux. You can provision thousands of EC2 instances on demand.
You also configure what software you want running on the instance. EC2 instances are also resizable. 
You might start with a small instance, realize the application you are running is starting to max out that server, 
and then you can give that instance more memory and more CPU. Which is what we call vertically scaling an instance.
You also control the networking aspect of EC2. So what type of requests make it to your server and if they are publicly or privately accessible is something you decide.


=> Amazon EC2 instance types are optimized for different tasks. When selecting an instance type, consider the specific needs of your workloads and applications. 
This might include requirements for compute, memory, or storage capabilities.

- General Purpose Instances provide a good balance of compute, memory, and networking resources, 
and can be used for a variety of diverse workloads like web service or code repositories.
Suppose that you have an application in which the resource needs for compute, memory, and networking are roughly equivalent.
You might consider running it on a general purpose instance because the application does not require optimization in any single resource area.
Balances compute, memory, and networking resources
- Compute Optimized Instances are ideal for compute-intensive tasks like gaming servers, high performance computing or HPC, and even scientific modeling.
You can also use compute optimized instances for batch processing workloads that require processing many transactions in a single group.
Offers high-performance processors
- Memory Optimized Instances are good for memory-intensive tasks. 
It holds all the data and instructions that a central processing unit (CPU) needs to be able to complete actions. 
Before a computer program or application is able to run, it is loaded from storage into memory.
This preloading process gives the CPU direct access to the computer program.
Suppose that you have a workload that requires large amounts of data to be preloaded before running an application.
This scenario might be a high-performance database or a workload that involves performing real-time processing of a large amount of unstructured data.
In these types of use cases, consider using a memory optimized instance. 
Memory optimized instances enable you to run workloads with high memory needs and receive great performance.
Ideal for high-performance databases
- Accelerated Computing Instances are good for floating point number calculations, graphics processing, or data pattern matching, as they use hardware accelerators.
In computing, a hardware accelerator is a component that can expedite data processing. 
Accelerated computing instances are ideal for workloads such as graphics applications, game streaming, and application streaming.
- Storage Optimized Instances are good for workloads that require high performance for locally stored data.
In computing, the term input/output operations per second (IOPS) is a metric that measures the performance of a storage device.
It indicates how many different input or output operations a device can perform in one second.
Storage optimized instances are designed to deliver tens of thousands of low-latency, random IOPS to applications. 
You can think of input operations as data put into a system, such as records entered into a database. An output operation is data generated by a server. 
An example of output might be the analytics performed on the records in a database.
If you have an application that has a high IOPS requirement, a storage optimized instance can provide better performance 
over other instance types not optimized for this kind of use case.
Suitable for data warehousing applications


=> Amazon EC2 pricing
- On-Demand
On-Demand Instances are ideal for short-term, irregular workloads that cannot be interrupted. No upfront costs or minimum contracts apply.
The instances run continuously until you stop them, and you pay for only the compute time you use.
Sample use cases for On-Demand Instances include developing and testing applications and running applications that have unpredictable usage patterns.
On-Demand Instances are not recommended for workloads that last a year or longer because these workloads can experience greater cost savings using Reserved Instances.

- Amazon EC2 Savings Plans
AWS offers Savings Plans for several compute services, including Amazon EC2. 
Amazon EC2 Savings Plans enable you to reduce your compute costs by committing to a consistent amount of compute usage for a 1-year or 3-year term.
This term commitment results in savings of up to 66% over On-Demand costs.
Any usage up to the commitment is charged at the discounted plan rate (for example, $10 an hour). Any usage beyond the commitment is charged at regular On-Demand rates.
Later in this course, you will review AWS Cost Explorer, a tool that enables you to visualize, understand, and manage your AWS costs and usage over time.
If you are considering your options for Savings Plans, AWS Cost Explorer can analyze your Amazon EC2 usage over the past 7, 30, or 60 days.
AWS Cost Explorer also provides customized recommendations for Savings Plans.
These recommendations estimate how much you could save on your monthly Amazon EC2 costs, 
based on previous Amazon EC2 usage and the hourly commitment amount in a 1-year or 3-year plan.

- Reserved Instances
Reserved Instances are a billing discount applied to the use of On-Demand Instances in your account.
You can purchase Standard Reserved and Convertible Reserved Instances for a 1-year or 3-year term, and Scheduled Reserved Instances for a 1-year term. 
You realize greater cost savings with the 3-year option.
At the end of a Reserved Instance term, you can continue using the Amazon EC2 instance without interruption. 
However, you are charged On-Demand rates until you do one of the following:
Terminate the instance.
Purchase a new Reserved Instance that matches the instance attributes (instance type, Region, tenancy, and platform).

- Spot Instances
Spot Instances are ideal for workloads with flexible start and end times, or that can withstand interruptions.
Spot Instances use unused Amazon EC2 computing capacity and offer you cost savings at up to 90% off of On-Demand prices.
Suppose that you have a background processing job that can start and stop as needed (such as the data processing job for a customer survey).
You want to start and stop the processing job without affecting the overall operations of your business. 
If you make a Spot request and Amazon EC2 capacity is available, your Spot Instance launches.
However, if you make a Spot request and Amazon EC2 capacity is unavailable, the request is not successful until capacity becomes available. 
The unavailable capacity might delay the launch of your background processing job.
After you have launched a Spot Instance, if capacity is no longer available or demand for Spot Instances increases, your instance may be interrupted. 
This might not pose any issues for your background processing job.
However, in the earlier example of developing and testing applications, you would most likely want to avoid unexpected interruptions.
Therefore, choose a different EC2 instance type that is ideal for those tasks.

- Dedicated Hosts
Dedicated Hosts are physical servers with Amazon EC2 instance capacity that is fully dedicated to your use. 
You can use your existing per-socket, per-core, or per-VM software licenses to help maintain license compliance.
You can purchase On-Demand Dedicated Hosts and Dedicated Hosts Reservations. Of all the Amazon EC2 options that were covered, Dedicated Hosts are the most expensive.


=> Amazon EC2 Auto Scaling
Amazon EC2 Auto Scaling enables you to automatically add or remove Amazon EC2 instances in response to changing application demand.
By automatically scaling your instances in and out as needed, you are able to maintain a greater sense of application availability.
Within Amazon EC2 Auto Scaling, you can use two approaches: dynamic scaling and predictive scaling.
Dynamic scaling responds to changing demand. 
Predictive scaling automatically schedules the right number of Amazon EC2 instances based on predicted demand.
To scale faster, you can use dynamic scaling and predictive scaling together.
- Example: Amazon EC2 Auto Scaling
By adding Amazon EC2 Auto Scaling to an application, you can add new instances to the application when necessary and terminate them when no longer needed.
Suppose that you are preparing to launch an application on Amazon EC2 instances. 
When configuring the size of your Auto Scaling group, you might set the minimum number of Amazon EC2 instances at one.
This means that at all times, there must be at least one Amazon EC2 instance running.
- Minimum Capacity is the number of Amazon EC2 instances that launch immediately after you have created the Auto Scaling group.
 In this example, the Auto Scaling group has a minimum capacity of one Amazon EC2 instance.
- Desired Capacity. If you do not specify the desired number of Amazon EC2 instances in an Auto Scaling group, the desired capacity defaults to your minimum capacity.
- Maximum Capacity.
Because Amazon EC2 Auto Scaling uses Amazon EC2 instances, you pay for only the instances you use, when you use them.


=> Elastic Load Balancing
Elastic Load Balancing is the AWS service that automatically distributes incoming application traffic across multiple resources, such as Amazon EC2 instances. 
A load balancer acts as a single point of contact for all incoming web traffic to your Auto Scaling group. 
This means that as you add or remove Amazon EC2 instances in response to the amount of incoming traffic, these requests route to the load balancer first. 
Then, the requests spread across multiple resources that will handle them.
For example, if you have multiple Amazon EC2 instances, Elastic Load Balancing distributes the workload across the multiple instances 
so that no single instance has to carry the bulk of it. 
Although Elastic Load Balancing and Amazon EC2 Auto Scaling are separate services, they work together to help ensure that applications 
running in Amazon EC2 can provide high performance and availability. 
- Low-demand period
Here’s an example of how Elastic Load Balancing works. Suppose that a few customers have come to the coffee shop and are ready to place their orders. 
If only a few registers are open, this matches the demand of customers who need service. The coffee shop is less likely to have open registers with no customers. 
In this example, you can think of the registers as Amazon EC2 instances.
- High-demand period
Throughout the day, as the number of customers increases, the coffee shop opens more registers to accommodate them. 
In the diagram, the Auto Scaling group represents this.
Additionally, a coffee shop employee directs customers to the most appropriate register so that the number of requests can evenly distribute across the open registers. 
You can think of this coffee shop employee as a load balancer. 


=> Messaging and Queuing
- Amazon Simple Notification Service (Amazon SNS) is a publish/subscribe service. Using Amazon SNS topics, a publisher publishes messages to subscribers.
This is similar to the coffee shop; the cashier provides coffee orders to the barista who makes the drinks.
In Amazon SNS, subscribers can be web servers, email addresses, AWS Lambda functions, or several other options. 
- Publishing updates from a single topic: Suppose that the coffee shop has a single newsletter that includes updates from all areas of its business. 
It includes topics such as coupons, coffee trivia, and new products.
All of these topics are grouped because this is a single newsletter. 
All customers who subscribe to the newsletter receive updates about coupons, coffee trivia, and new products.
After a while, some customers express that they would prefer to receive separate newsletters for only the specific topics that interest them.
The coffee shop owners decide to try this approach.
- Publishing updates from multiple topics: Now, instead of having a single newsletter for all topics, the coffee shop has broken it up into three separate newsletters.
Each newsletter is devoted to a specific topic: coupons, coffee trivia, and new products.
Subscribers will now receive updates immediately for only the specific topics to which they have subscribed.
It is possible for subscribers to subscribe to a single topic or to multiple topics.
For example, the first customer subscribes to only the coupons topic, and the second subscriber subscribes to only the coffee trivia topic.
The third customer subscribes to both the coffee trivia and new products topics.

- Amazon Simple Queue Service (Amazon SQS) is a message queuing service. 
Using Amazon SQS, you can send, store, and receive messages between software components, without losing messages or requiring other services to be available.
In Amazon SQS, an application sends messages into a queue. A user or service retrieves a message from the queue, processes it, and then deletes it from the queue.


=> Additional AWS Compute Options
- Serverless computing
The term “serverless” means that your code runs on servers, but you do not need to provision or manage these servers. 
With serverless computing, you can focus more on innovating new products and features instead of maintaining servers.
Another benefit of serverless computing is the flexibility to scale serverless applications automatically. 
Serverless computing can adjust the applications' capacity by modifying the units of consumptions, such as throughput and memory. 
An AWS service for serverless computing is AWS Lambda.

- AWS Lambda
AWS Lambda is a service that lets you run code without needing to provision or manage servers. 
While using AWS Lambda, you pay only for the compute time that you consume. Charges apply only when your code is running. 
You can also run code for virtually any type of application or backend service, all with zero administration. 
For example, a simple Lambda function might involve automatically resizing uploaded images to the AWS Cloud. 
In this case, the function triggers when uploading a new image.

- Amazon Elastic Container Service (Amazon ECS)
Amazon Elastic Container Service (Amazon ECS) is a highly scalable, high-performance container management system that enables you 
to run and scale containerized applications on AWS. 
Amazon ECS supports Docker containers. Docker is a software platform that enables you to build, test, and deploy applications quickly.
AWS supports the use of open-source Docker Community Edition and subscription-based Docker Enterprise Edition. 
With Amazon ECS, you can use API calls to launch and stop Docker-enabled applications.

- Amazon Elastic Kubernetes Service (Amazon EKS)
Amazon Elastic Kubernetes Service (Amazon EKS) is a fully managed service that you can use to run Kubernetes on AWS. 
Kubernetes is open-source software that enables you to deploy and manage containerized applications at scale. 
A large community of volunteers maintains Kubernetes, and AWS actively works together with the Kubernetes community. 
As new features and functionalities release for Kubernetes applications, you can easily apply these updates to your applications managed by Amazon EKS.

- AWS Fargate
AWS Fargate is a serverless compute engine for containers. It works with both Amazon ECS and Amazon EKS.
When using AWS Fargate, you do not need to provision or manage servers. AWS Fargate manages your server infrastructure for you.
You can focus more on innovating and developing your applications, and you pay only for the resources that are required to run your containers.


=> Compute Services Differences
Amazon EC2
- Host traditional applications.
- Have full access to the OS.

AWS Lambda
- Host short running functions.
- Service-oriented applications.
- Event driven applications.
- No provisioning or managing servers.

Amazon ECS or Amazon EKS
- Run Docker container-based workloads on AWS.
- Can run in EC2 instances that you manage or in a serveless environment like AWS Fargate that will be managed for you by AWS.


=> ADDITIONAL RESOURCES

Compute on AWS - https://aws.amazon.com/products/compute
AWS Compute Blog - https://aws.amazon.com/blogs/compute/
AWS Compute Services - https://docs.aws.amazon.com/whitepapers/latest/aws-overview/compute-services.html
Hands-On Tutorials: Compute - 
https://aws.amazon.com/getting-started/hands-on/?awsf.getting-started-category=category%23compute&awsf.getting-started-content-type=content-type%23hands-on
Category Deep Dive: Serverless - https://aws.amazon.com/getting-started/deep-dive-serverless/


=> QUIZ

1 - Which AWS service is the best choice for publishing messages to subscribers?

a) Amazon Simple Queue Service (Amazon SQS)

b) Amazon EC2 Auto Scaling

c) Amazon Simple Notification Service (Amazon SNS)

d) Elastic Load Balancing

The correct response option is Amazon Simple Notification Service (Amazon SNS).


Amazon SNS is a publish/subscribe service. Using Amazon SNS topics, a publisher publishes messages to subscribers.

The other response options are incorrect because: 

Amazon Simple Queue Service (Amazon SQS) is a message queuing service. It does not use the message subscription and topic model that is involved with Amazon SNS.
Amazon EC2 Auto Scaling enables you to automatically add or remove Amazon EC2 instances in response to changing application demand.
Elastic Load Balancing is the AWS service that automatically distributes incoming application traffic across multiple resources, such as Amazon EC2 instances.


2 - You want to use an Amazon EC2 instance for a batch processing workload. What would be the best Amazon EC2 instance type to use?

a) General purpose

b) Memory optimized

c) Compute optimized

d) Storage optimized

The correct response option is Compute optimized.


The other response options are incorrect because:

General purpose instances provide a balance of compute, memory, and networking resources. 
This instance family would not be the best choice for the application in this scenario.
Compute optimized instances are more well suited for batch processing workloads than general purpose instances.
Memory optimized instances are more ideal for workloads that process large datasets in memory, such as high-performance databases.
Storage optimized instances are designed for workloads that require high, sequential read and write access to large datasets on local storage.
The question does not specify the size of data that will be processed. Batch processing involves processing data in groups. 
A compute optimized instance is ideal for this type of workload, which would benefit from a high-performance processor.


3 - What are the contract length options for Amazon EC2 Reserved Instances? (Select TWO.)

a) 1 year

b) 2 years

c) 3 years

d) 4 years

e) 5 years

The two correct response options are:

1 year
3 years
Reserved Instances require a commitment of either 1 year or 3 years. The 3-year option offers a larger discount.


4 - You have a workload that will run for a total of 6 months and can withstand interruptions. What would be the most cost-efficient Amazon EC2 purchasing option?

a) Reserved Instance

b) Spot Instance

c) Dedicated Instance

d) On-Demand Instance

The correct response option is Spot Instance.


The other response options are incorrect because:

Reserved Instances require a contract length of either 1 year or 3 years. The workload in this scenario will only be running for 6 months.
Dedicated Instances run in a virtual private cloud (VPC) on hardware that is dedicated to a single customer. 
They have a higher cost than the other response options, which run on shared hardware.
On-Demand Instances fulfill the requirements of running for only 6 months and withstanding interruptions.
However, a Spot Instance would be the best choice because it does not require a minimum contract length, 
is able to withstand interruptions, and costs less than an On-Demand Instance.


5 - Which process is an example of Elastic Load Balancing?

a) Ensuring that no single Amazon EC2 instance has to carry the full workload on its own

b) Removing unneeded Amazon EC2 instances when demand is low

c) Adding a second Amazon EC2 instance during an online store’s popular sale

d) Automatically adjusting the number of Amazon EC2 instances to meet demand

The correct response option is Ensuring that no single Amazon EC2 instance has to carry the full workload on its own.


Elastic Load Balancing is the AWS service that automatically distributes incoming application traffic across multiple resources, such as Amazon EC2 instances. 
This helps to ensure that no single resource becomes overutilized.
The other response options are all examples of Auto Scaling.


6 - You want to deploy and manage containerized applications. Which service should you use?

a) AWS Lambda

b) Amazon Simple Notification Service (Amazon SNS)

c) Amazon Simple Queue Service (Amazon SQS)

d) Amazon Elastic Kubernetes Service (Amazon EKS)

The correct response option is Amazon Elastic Kubernetes Service (Amazon EKS).


Amazon EKS is a fully managed Kubernetes service. Kubernetes is open-source software that enables you to deploy and manage containerized applications at scale.

The other response options are incorrect because:

AWS Lambda is a service that lets you run code without provisioning or managing servers.
Amazon Simple Queue Service (Amazon SQS) is a service that enables you to send, store, and receive messages between software components through a queue.
Amazon Simple Notification Service (Amazon SNS) is a publish/subscribe service. Using Amazon SNS topics, a publisher publishes messages to subscribers.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------

MODULE 3
Summarize the benefits of the AWS Global Infrastructure.
Describe the basic concept of Availability Zones.
Describe the benefits of Amazon CloudFront and edge locations.
Compare different methods for provisioning AWS services.

=> Region
A Region is a geographical area that contains AWS resources.
Regions are geographically isolated areas, where you can access services needed to run your enterprise.


=> Selecting a Region

- Compliance with data governance and legal requirements
Depending on your company and location, you might need to run your data out of specific areas. 
For example, if your company requires all of its data to reside within the boundaries of the UK, you would choose the London Region. 
Not all companies have location-specific data regulations, so you might need to focus more on the other three factors.

- Proximity to your customers
Selecting a Region that is close to your customers will help you to get content to them faster.
For example, your company is based in Washington, DC, and many of your customers live in Singapore.
You might consider running your infrastructure in the Northern Virginia Region to be close to company headquarters, and run your applications from the Singapore Region.

- Available services within a Region
Sometimes, the closest Region might not have all the features that you want to offer to customers.
AWS is frequently innovating by creating new services and expanding on features within existing services.
However, making new services available around the world sometimes requires AWS to build out physical hardware one Region at a time. 
Suppose that your developers want to build an application that uses Amazon Braket (AWS quantum computing platform).
As of this course, Amazon Braket is not yet available in every AWS Region around the world, 
so your developers would have to run it in one of the Regions that already offers it.

- Pricing
Suppose that you are considering running applications in both the United States and Brazil.
The way Brazil’s tax structure is set up, it might cost 50% more to run the same workload out of the São Paulo Region compared to the Oregon Region.
You will learn in more detail that several factors determine pricing, but for now know that the cost of services can vary from Region to Region.


=> Availability Zones
An Availability Zone is a single data center or a group of data centers within a Region. Availability Zones are located tens of miles apart from each other.
This is close enough to have low latency (the time between when content requested and received) between Availability Zones.
However, if a disaster occurs in one part of the Region, they are distant enough to reduce the chance that multiple Availability Zones are affected.


=> Running Amazon EC2 instances in multiple Availability Zones
- Amazon EC2 instance in a single Availability Zone
Suppose that you’re running an application on a single Amazon EC2 instance in the Northern California Region.
The instance is running in the us-west-1a Availability Zone. If us-west-1a were to fail, you would lose your instance. 
- Amazon EC2 instances in multiple Availability Zones
A best practice is to run applications across at least two Availability Zones in a Region. 
In this example, you might choose to run a second Amazon EC2 instance in us-west-1b.
- Availability Zone failure
If us-west-1a were to fail, your application would still be running in us-west-1b.


=> Edge Locations
An edge location is a site that Amazon CloudFront uses to store cached copies of your content closer to your customers for faster delivery.

Amazon CloudFront is a service that helps deliver data, video, applications, and APIs to customers around the world with low latency and high transfer speeds.
Amazon CloudFront uses what are called Edge locations, all around the world, to help accelerate communication with users, no matter where they are. 

AWS Outposts: AWS will basically install a fully operational mini Region, right inside your own data center.
That's owned and operated by AWS, using 100% of AWS functionality, but isolated within your own building.
It's not a solution most customers need, but if you have specific problems that can only be solved by staying in your own building, we understand, AWS Outposts can help. 


=> Key Points
Number one, Regions are geographically isolated areas, where you can access services needed to run your enterprise.
Number two, Regions contain Availability Zones, that allow you to run across physically separated buildings, tens of miles of separation, 
while keeping your application logically unified.
Availability Zones help you solve high availability and disaster recovery scenarios, without any additional effort on your part.
Number three, AWS Edge locations run Amazon CloudFront to help get content closer to your customers, no matter where they are in the world.


=> Ways to interact with AWS services
- AWS MANAGEMENT CONSOLE
The AWS Management Console is a web-based interface for accessing and managing AWS services.
You can quickly access recently used services and search for other services by name, keyword, or acronym.
The console includes wizards and automated workflows that can simplify the process of completing tasks.
You can also use the AWS Console mobile application to perform tasks such as monitoring resources, viewing alarms, and accessing billing information.
Multiple identities can stay logged into the AWS Console mobile app at the same time.

- AWS COMMAND LINE INTERFACE
To save time when making API requests, you can use the AWS Command Line Interface (AWS CLI).
AWS CLI enables you to control multiple AWS services directly from the command line within one tool.
AWS CLI is available for users on Windows, macOS, and Linux. 
By using AWS CLI, you can automate the actions that your services and applications perform through scripts.
For example, you can use commands to launch an Amazon EC2 instance, connect an Amazon EC2 instance to a specific Auto Scaling group, and more.

- SOFTWARE DEVELOPMENT KITS
Another option for accessing and managing AWS services is the software development kits (SDKs).
SDKs make it easier for you to use AWS services through an API designed for your programming language or platform.
SDKs enable you to use AWS services with your existing applications or create entirely new applications that will run on AWS.
To help you get started with using SDKs, AWS provides documentation and sample code for each supported programming language.
Supported programming languages include C++, Java, .NET, and more.


=> AWS Elastic Beanstalk
With AWS Elastic Beanstalk, you provide code and configuration settings, and Elastic Beanstalk deploys the resources necessary to perform the following tasks:

- Adjust capacity
- Load balancing
- Automatic scaling
- Application health monitoring


=> AWS CloudFormation
With AWS CloudFormation, you can treat your infrastructure as code.
This means that you can build an environment by writing lines of code instead of using the AWS Management Console to individually provision resources.
AWS CloudFormation provisions your resources in a safe, repeatable manner, enabling you to frequently build your infrastructure and 
applications without having to perform manual actions.
It determines the right operations to perform when managing your stack and rolls back changes automatically if it detects errors.


=> ADDITIONAL RESOURCES
- Global Infrastructure - https://aws.amazon.com/about-aws/global-infrastructure/
- Interactive map of the AWS Global Infrastructure - https://www.infrastructure.aws/
- Regions and Availability Zones - https://aws.amazon.com/about-aws/global-infrastructure/regions_az
- AWS Networking and Content Delivery Blog - https://aws.amazon.com/blogs/networking-and-content-delivery/
- Tools to Build on AWS - https://aws.amazon.com/tools/


=> QUIZ

1 - Which statement best describes an Availability Zone?

a) A geographical area that contains AWS resources

b) A single data center or group of data centers within a Region

c) A data center that an AWS service uses to perform service-specific operations

d) A service that you can use to run AWS infrastructure within your own on-premises data center in a hybrid approach

The correct response option is A single data center or group of data centers within a Region.


The other response options are incorrect because:

A Region is a geographical area that contains AWS resources.
An edge location is a data center that an AWS service uses to perform service-specific operations. Edge locations are examined in the next section of this module.
AWS Outposts is a service that you can use to run AWS infrastructure, services, and tools in your own on-premises data center in a hybrid approach. 
AWS Outposts is explored later in this module.


2 - Which statement is TRUE for the AWS global infrastructure?

a) A Region consists of a single Availability Zone.

b) An Availability Zone consists of two or more Regions.

c) A Region consists of two or more Availability Zones.

d) An Availability Zone consists of a single Region.

The correct response option is A Region consists of two or more Availability Zones.

For example, the South America (São Paulo) Region is sa-east-1. It includes three Availability Zones: sa-east-1a, sa-east-1b, and sa-east-1c.


3 - Which factors should be considered when selecting a Region? (Select TWO.)

a) Compliance with data governance and legal requirements

b) Proximity to your customers

c) Access to 24/7 technical support

d) Ability to assign custom permissions to different users

e) Access to the AWS Command Line Interface (AWS CLI)

The correct two response options are:

Compliance with data governance and legal requirements
Proximity to your customers
Two other factors to consider when selecting a Region are pricing and the services that are available in a Region.


The other response options are incorrect because:

The level of support that you choose is not determined by Region. AWS Support plans are explored later in this course.
Assigning custom permissions to different users is a feature that is possible in all AWS Regions.
The AWS Command Line Interface (AWS CLI) is available in all AWS Regions.


4 - Which statement best describes Amazon CloudFront?

a) A service that enables you to run infrastructure in a hybrid cloud approach

b) A serverless compute engine for containers

c) A service that enables you to send and receive messages between software components through a queue

d) A global content delivery service

The correct response option is A global content delivery service.


Amazon CloudFront is a content delivery service.
It uses a network of edge locations to cache content and deliver content to customers all over the world.
When content is cached, it is stored locally as a copy. This content might be video files, photos, webpages, and so on.

The other response options are incorrect because:

AWS Outposts is a service that enables you to run infrastructure in a hybrid cloud approach.
AWS Fargate is a serverless compute engine for containers.
Amazon Simple Queue Service (Amazon SQS) is a service that enables you to send, store, and receive messages between software components through a queue.


5 - Which site does Amazon CloudFront use to cache copies of content for faster delivery to users at any location?

a) Region

b) Availability Zone

c) Edge location

d) Origin

The correct response option is Edge location.


The other response options are incorrect because:

A Region is a separate geographical location with multiple locations that are isolated from each other.
An Availability Zone is a fully isolated portion of the AWS global infrastructure.
An origin is the server from which CloudFront gets your files.
Examples of CloudFront origins include Amazon Simple Storage Service (Amazon S3) buckets and web servers.
Note: Amazon S3 is explored later in this course.


6 - Which action can you perform with AWS Outposts?

a) Automate actions for AWS services and applications through scripts.

b) Access wizards and automated workflows to perform tasks in AWS services.

c) Develop AWS applications in supported programming languages.

d) Extend AWS infrastructure and services to your on-premises data center.

The correct response option is Extend AWS infrastructure and services to your on-premises data center.


The other response options are incorrect because:

The AWS Command Line Interface (AWS CLI) is used to automate actions for AWS services and applications through scripts.
The AWS Management Console includes wizards and workflows that you can use to complete tasks in AWS services.
Software development kits (SDKs) enable you to develop AWS applications in supported programming languages.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------

MODULE 4
Describe the basic concepts of networking.
Describe the difference between public and private networking resources. 
Explain a virtual private gateway using a real life scenario. 
Explain a virtual private network (VPN) using a real life scenario.
Describe the benefit of AWS Direct Connect. 
Describe the benefit of hybrid deployments. 
Describe the layers of security used in an IT strategy.
Describe the services customers use to interact with the AWS global network.

=> Amazon Virtual Private Cloud (VPC)
A VPC lets you provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define.
These resources can be public facing so they have access to the internet, or private with no internet access, 
usually for backend services like databases or application servers.
The public and private grouping of resources are known as subnets and they are ranges of IP addresses in your VPC.

Amazon VPC enables you to provision an isolated section of the AWS Cloud. In this isolated section, you can launch resources in a virtual network that you define.
Within a virtual private cloud (VPC), you can organize your resources into subnets.
A subnet is a section of a VPC that can contain resources such as Amazon EC2 instances.


=> Internet gateway
It allows public traffic from the internet to access your VPC.
An internet gateway is a connection between a VPC and the internet. 


=> Virtual private gateway
To access private resources in a VPC, you can use a virtual private gateway.
The virtual private gateway is the component that allows protected internet traffic to enter into the VPC.
A virtual private gateway enables you to establish a virtual private network (VPN) connection between your VPC and a private network, 
such as an on-premises data center or internal corporate network.
A virtual private gateway allows traffic into the VPC only if it is coming from an approved network.


=> AWS Direct Connect
AWS Direct Connect is a service that enables you to establish a dedicated private connection between your data center and a VPC.
The private connection that AWS Direct Connect provides helps you to reduce network costs and 
increase the amount of bandwidth that can travel through your network.


=> Security Group
Statefull (save the state to avoid do security check all the time).
Make the security for the instance (like EC2 instance).

A security group is a virtual firewall that controls inbound and outbound traffic for an Amazon EC2 instance.
By default, a security group denies all inbound traffic and allows all outbound traffic.
You can add custom rules to configure which traffic to allow or deny.
If you have multiple Amazon EC2 instances within a subnet, you can associate them with the same security group or use different security groups for each instance. 


=> Network Access Control Lists (ACL)
Stateless (always do the security check).
Make the security for the Subnet.

A network access control list (ACL) is a virtual firewall that controls inbound and outbound traffic at the subnet level.
Each AWS account includes a default network ACL. When configuring your VPC, you can use your account’s default network ACL or create custom network ACLs. 
By default, your account’s default network ACL allows all inbound and outbound traffic, but you can modify it by adding your own rules.
For custom network ACLs, all inbound and outbound traffic is denied until you add rules to specify which traffic to allow.
Additionally, all network ACLs have an explicit deny rule.
This rule ensures that if a packet doesn’t match any of the other rules on the list, the packet is denied.


Both network ACLs and security groups enable you to configure custom rules for the traffic in your VPC.
As you continue to learn more about AWS security and networking, make sure to understand the differences between network ACLs and security groups.


=> Public subnets contain resources that need to be accessible by the public, such as an online store’s website.


=> Private subnets contain resources that should be accessible only through your private network, 
such as a database that contains customers’ personal information and order histories.


In a VPC, subnets can communicate with each other.
For example, you might have an application that involves Amazon EC2 instances 
in a public subnet communicating with databases that are located in a private subnet.


=> Network traffic in a VPC
When a customer requests data from an application hosted in the AWS Cloud, this request is sent as a packet. 
A packet is a unit of data sent over the internet or a network.
It enters into a VPC through an internet gateway. Before a packet can enter into a subnet or exit from a subnet, it checks for permissions. 
These permissions indicate who sent the packet and how the packet is trying to communicate with the resources in a subnet.
The VPC component that checks packet permissions for subnets is a network access control list (ACL).

 
=> Knowledge check
1 - For this review exercise, imagine that your company is launching an online photo storage application.
Help determine which VPC component should be used for certain aspects of the application.
In the following, match each part of the application to the correct VPC component.


Isolate databases containing customers' personal information.                           ->  Private subnet
Create a VPN connection between the VPC and the internal corporate network.             ->  Virtual private gateway
Support the customer-facing website.                                                    ->  Public subnet
Establish a dedicated connection between the on-premises data center and the VPC.       ->  AWS Direct Connect


2 - Which statement best describes an AWS account’s default network access control list?

a) It is stateless and denies all inbound and outbound traffic.

b) It is stateful and allows all inbound and outbound traffic.

c) It is stateless and allows all inbound and outbound traffic.

d) It is stateful and denies all inbound and outbound traffic.

The correct response option is It is stateless and allows all inbound and outbound traffic.

Network access control lists (ACLs) perform stateless packet filtering. 
They remember nothing and check packets that cross the subnet border each way: inbound and outbound.
Each AWS account includes a default network ACL. When configuring your VPC, you can use your account’s default network ACL or create custom network ACLs.

By default, your account’s default network ACL allows all inbound and outbound traffic, but you can modify it by adding your own rules.
For custom network ACLs, all inbound and outbound traffic is denied until you add rules to specify which traffic should be allowed.
Additionally, all network ACLs have an explicit deny rule.
This rule ensures that if a packet doesn’t match any of the other rules on the list, the packet is denied.


=> Domain Name System (DNS)
DNS resolution involves a customer DNS resolver communicating with a company DNS server.
DNS resolution is the process of translating a domain name to an IP address.


=> Amazon Route 53
Amazon Route 53 is a DNS web service. It gives developers and businesses a reliable way to route end users to internet applications hosted in AWS.
Amazon Route 53 connects user requests to infrastructure running in AWS (such as Amazon EC2 instances and load balancers).
It can route users to infrastructure outside of AWS.

Another feature of Route 53 is the ability to manage the DNS records for domain names.
You can register new domain names directly in Route 53.
You can also transfer DNS records for existing domain names managed by other domain registrars.
This enables you to manage all of your domain names within a single location.


=> QUIZ

1 - Your company has an application that uses Amazon EC2 instances to run the customer-facing website and Amazon RDS database instances 
to store customers’ personal information. How should the developer configure the VPC according to best practices?

a) Place the Amazon EC2 instances in a private subnet and the Amazon RDS database instances in a public subnet.

b) Place the Amazon EC2 instances in a public subnet and the Amazon RDS database instances in a private subnet.

c) Place the Amazon EC2 instances and the Amazon RDS database instances in a public subnet.

d) Place the Amazon EC2 instances and the Amazon RDS database instances in a private subnet.

The correct response option is Place the Amazon EC2 instances in a public subnet and the Amazon RDS databases instances in a private subnet.


A subnet is a section of a VPC in which you can group resources based on security or operational needs. Subnets can be public or private.
Public subnets contain resources that need to be accessible by the public, such as an online store’s website.
Private subnets contain resources that should be accessible only through your private network, 
such as a database that contains customers’ personal information and order histories.


2 - Which component or service can be used to establish a private dedicated connection between your company’s data center and AWS?

a) Private subnet

b) DNS

c) AWS Direct Connect

d) Amazon CloudFront

The correct response option is AWS Direct Connect.


The other response options are incorrect because:

A private subnet is a section of a VPC in which you can group resources that should be accessed only through your private network.
Although it is private, it is not used for establishing a connection between a data center and AWS.
DNS stands for Domain Name System, which is a directory used for matching domain names to IP addresses.
Amazon CloudFront is a content delivery service.
You can use CloudFront to store cached copies of your content at edge locations that are close to your customers.


3 - Which statement best describes security groups?

a) They are stateful and deny all inbound traffic by default.

b) They are stateful and allow all inbound traffic by default.

c) They are stateless and deny all inbound traffic by default.

d) They are stateless and allow all inbound traffic by default.

The correct response option is Security groups are stateful and deny all inbound traffic by default.


Security groups are stateful. This means that they use previous traffic patterns and flows when evaluating new requests for an instance.
By default, security groups deny all inbound traffic, but you can add custom rules to fit your operational and security needs.


4 - Which component is used to connect a VPC to the internet?

a) Public subnet

b) Edge location

c) Security group

d) Internet gateway

The correct response option is Internet gateway.


The other response options are incorrect because:

A public subnet is a section of a VPC that contains public-facing resources.
An edge location is a site that Amazon CloudFront uses to store cached copies of your content for faster delivery to customers.
A security group is a virtual firewall that controls inbound and outbound traffic for an Amazon EC2 instance.


5 - Which service is used to manage the DNS records for domain names?

a) Amazon Virtual Private Cloud

b) AWS Direct Connect

c) Amazon CloudFront

d) Amazon Route 53

The correct response option is Amazon Route 53.

Amazon Route 53 is a DNS web service. It gives developers and businesses a reliable way to route end users to internet applications that host in AWS.
Another feature of Route 53 is the ability to manage the DNS records for domain names. 
You can transfer DNS records for existing domain names managed by other domain registrars. 
You can also register new domain names directly in Route 53.


The other response options are incorrect because:

Amazon Virtual Private Cloud (Amazon VPC) is a service that enables you to provision an isolated section of the AWS Cloud.
In this isolated section, you can launch resources in a virtual network that you define.
AWS Direct Connect is a service that enables you to establish a dedicated private connection between your data center and VPC.  
Amazon CloudFront is a content delivery service. It uses a network of edge locations to cache content and deliver content to customers all over the world.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------

MODULE 5
Summarize the basic concept of storage and databases.
Describe the benefits of Amazon Elastic Block Store (Amazon EBS).
Describe the benefits of Amazon Simple Storage Service (Amazon S3).
Describe the benefits of Amazon Elastic File System (Amazon EFS).
Summarize various storage solutions.
Describe the benefits of Amazon Relational Database Service (Amazon RDS).
Describe the benefits of Amazon DynamoDB.
Summarize various database services.

=> Instance stores
Block-level storage volumes behave like physical hard drives.
An instance store provides temporary block-level storage for an Amazon EC2 instance.
An instance store is disk storage that is physically attached to the host computer for an EC2 instance, 
and therefore has the same lifespan as the instance. When the instance is terminated, you lose any data in the instance store.


Amazon EC2 instances are virtual servers. 
If you start an instance from a stopped state, the instance might start on another host, where the previously used instance store volume does not exist.
Therefore, AWS recommends instance stores for use cases that involve temporary data that you do not need in the long term.


=> Amazon Elastic Block Store (Amazon EBS)
It is a service that provides block-level storage volumes that you can use with Amazon EC2 instances.
If you stop or terminate an Amazon EC2 instance, all the data on the attached EBS volume remains available.
To create an EBS volume, you define the configuration (such as volume size and type) and provision it.
After you create an EBS volume, it can attach to an Amazon EC2 instance.

Because EBS volumes are for data that needs to persist, it’s important to back up the data.
You can take incremental backups of EBS volumes by creating Amazon EBS snapshots.


=> Amazon EBS snapshots
An EBS snapshot is an incremental backup. This means that the first backup taken of a volume copies all the data.
For subsequent backups, only the blocks of data that have changed since the most recent snapshot are saved. 

Incremental backups are different from full backups, in which all the data in a storage volume copies each time a backup occurs.
The full backup includes data that has not changed since the most recent backup.


- Instance Stores
Best for temporary data that is not kept long term
When stopping or terminating an EC2 instance, data is deleted

- Amazon ESB Volumes
Best for data that requires retention
When stopping or terminating an EC2 instance, data remains available


=> Object storage
In object storage, each object consists of data, metadata, and a key.
The data might be an image, video, text document, or any other type of file.
Metadata contains information about what the data is, how it is used, the object size, and so on. An object’s key is its unique identifier.

Recall that when you modify a file in block storage, only the pieces that are changed are updated.
When a file in object storage is modified, the entire object is updated.


=> Amazon Simple Storage Service (Amazon S3)
Amazon Simple Storage Service (Amazon S3) is a service that provides object-level storage. Amazon S3 stores data as objects in buckets.
You can upload any type of file to Amazon S3, such as images, videos, text files, and so on.
For example, you might use Amazon S3 to store backup files, media files for a website, or archived documents.
Amazon S3 offers unlimited storage space. The maximum file size for an object in Amazon S3 is 5 TB.
When you upload a file to Amazon S3, you can set permissions to control visibility and access to it.
You can also use the Amazon S3 versioning feature to track changes to your objects over time.


=> Amazon S3 Storage Classes

With Amazon S3, you pay only for what you use. You can choose from a range of storage classes to select a fit for your business and cost needs.
When selecting an Amazon S3 storage class, consider these two factors:
    How often you plan to retrieve your data
    How available you need your data to be


- S3 Standard
    Designed for frequently accessed data
    Stores data in a minimum of three Availability Zones
S3 Standard provides high availability for objects. 
This makes it a good choice for a wide range of use cases, such as websites, content distribution, and data analytics. 
S3 Standard has a higher cost than other storage classes intended for infrequently accessed data and archival storage.

- S3 Standard-Infrequent Access (S3 Standard-IA)
    Ideal for infrequently accessed data
    Similar to S3 Standard but has a lower storage price and higher retrieval price
S3 Standard-IA is ideal for data infrequently accessed but requires high availability when needed.
Both S3 Standard and S3 Standard-IA store data in a minimum of three Availability Zones.
S3 Standard-IA provides the same level of availability as S3 Standard but with a lower storage price and a higher retrieval price.

- S3 One Zone-Infrequent Access (S3 One Zone-IA)
    Stores data in a single Availability Zone
    Has a lower storage price than S3 Standard-IA
Compared to S3 Standard and S3 Standard-IA, which store data in a minimum of three Availability Zones, S3 One Zone-IA stores data in a single Availability Zone. 
This makes it a good storage class to consider if the following conditions apply:
    You want to save costs on storage.
    You can easily reproduce your data in the event of an Availability Zone failure.

- S3 Intelligent-Tiering
    Ideal for data with unknown or changing access patterns
    Requires a small monthly monitoring and automation fee per object
In the S3 Intelligent-Tiering storage class, Amazon S3 monitors objects’ access patterns.
If you haven’t accessed an object for 30 consecutive days, Amazon S3 automatically moves it to the infrequent access tier, S3 Standard-IA.
If you access an object in the infrequent access tier, Amazon S3 automatically moves it to the frequent access tier, S3 Standard.

- S3 Glacier
    Low-cost storage designed for data archiving
    Able to retrieve objects within a few minutes to hours
S3 Glacier is a low-cost storage class that is ideal for data archiving. 
For example, you might use this storage class to store archived customer records or older photos and video files.

- S3 Glacier Deep Archive
    Lowest-cost object storage class ideal for archiving
    Able to retrieve objects within 12 hours
When deciding between Amazon S3 Glacier and Amazon S3 Glacier Deep Archive, consider how quickly you need to retrieve archived objects.
You can retrieve objects stored in the S3 Glacier storage class within a few minutes to a few hours.
By comparison, you can retrieve objects stored in the S3 Glacier Deep Archive storage class within 12 hours.


=> Knowledge check

1 - You want to store data that is infrequently accessed but must be immediately available when needed. Which Amazon S3 storage class should you use?

a) S3 Intelligent-Tiering

b) S3 Glacier Deep Archive

c) S3 Standard-IA

d) S3 Glacier

The correct response option is S3 Standard-IA.

The S3 Standard-IA storage class is ideal for data that is infrequently accessed but requires high availability when needed.
Both S3 Standard and S3 Standard-IA store data in a minimum of three Availability Zones.
S3 Standard-IA provides the same level of availability as S3 Standard but at a lower storage price.


The other response options are incorrect because:

In the S3 Intelligent-Tiering storage class, Amazon S3 monitors objects’ access patterns. 
If you haven’t accessed an object for 30 consecutive days, Amazon S3 automatically moves it to the infrequent access tier, S3 Standard-IA.
If you access an object in the infrequent access tier, Amazon S3 automatically moves it to the frequent access tier, S3 Standard.

S3 Glacier and S3 Glacier Deep Archive are low-cost storage classes that are ideal for data archiving.
They would not be the best choice for this scenario, which requires high availability.
You can retrieve objects stored in the S3 Glacier storage class within a few minutes to a few hours.
By comparison, you can retrieve objects stored in the S3 Glacier Deep Archive storage class within 12 hours.


=> Comparing Amazon EBS and Amazon S3
- Amazon EBS
    Size up to 16TiB.
    Survive the termination of their Amazon EC2 instances.
    Solid state by default.
    HDD options.
    Only the pieces that are changed in the file are updated.
    Complex read/write/change operations.

- Amazon S3
    Unlimited storage.
    Individual objects up to 5TBs.
    Write once/read many.
    99.999999999% durability.
    Web enabled.
    Regionally distributed.
    Offers cost savings.
    Serverless.


=> File storage
In file storage, multiple clients (such as users, applications, servers, and so on) can access data that is stored in shared file folders.
In this approach, a storage server uses block storage with a local file system to organize files. Clients access data through file paths.
Compared to block storage and object storage, file storage is ideal for use cases in which 
a large number of services and resources need to access the same data at the same time.


=> Amazon Elastic File System (Amazon EFS)
It is a scalable file system used with AWS Cloud services and on-premises resources.
As you add and remove files, Amazon EFS grows and shrinks automatically. It can scale on demand to petabytes without disrupting applications. 


=> Comparing Amazon EBS and Amazon EFS
- Amazon EBS
    An Amazon EBS volume stores data in a single Availability Zone. 
    Volumes attach to EC2 instance.
    Availability Zone level resource.
    Need to be in the same Availability Zone to attach EC2 instances.
    Volumes do not automatically scale.

- Amazon EFS
    Amazon EFS is a regional service. It stores data in and across multiple Availability Zones. 
    Multiple instances reading and writing simultaneously.
    Linux file system.
    Regional resource.
    Automatically scale.


=> Amazon Relational Database Service (Amazon RDS) 
It is a service that enables you to run relational databases in the AWS Cloud.
Amazon RDS is a managed service that automates tasks such as hardware provisioning, database setup, patching, and backups.
With these capabilities, you can spend less time completing administrative tasks and more time using data to innovate your applications.
You can integrate Amazon RDS with other services to fulfill your business and operational needs, such as using AWS Lambda 
to query your database from a serverless application.

Amazon RDS provides a number of different security options. 
Many Amazon RDS database engines offer encryption at rest (protecting data while it is stored) and encryption in transit 
(protecting data while it is being sent and received).


=> Amazon RDS database engines
Amazon RDS is available on six database engines, which optimize for memory, performance, or input/output (I/O). Supported database engines include:

    Amazon Aurora
    PostgreSQL
    MySQL
    MariaDB
    Oracle Database
    Microsoft SQL Server


=> Amazon Aurora
It is an enterprise-class relational database. It is compatible with MySQL and PostgreSQL relational databases.
It is up to five times faster than standard MySQL databases and up to three times faster than standard PostgreSQL databases.
Amazon Aurora helps to reduce your database costs by reducing unnecessary input/output (I/O) operations, 
while ensuring that your database resources remain reliable and available.
Consider Amazon Aurora if your workloads require high availability.
It replicates six copies of your data across three Availability Zones and continuously backs up your data to Amazon S3.


=> Amazon DynamoDB
    Non-relational, NoSQL database.
    Purpose built.
    Millisecond response time.
    Fully managed.
    Highly scalable.

DynamoDB is serverless, which means that you do not have to provision, patch, or manage servers.
You also do not have to install, maintain, or operate software.


=> Comparing Amazon RDS and Amazon DynamoDB
- Amazon RDS
    Automatic high availability.
    Customer ownership of data.
    Customer ownership of schema.
    Customer control of network.

- Amazon DynamoDB
    Key-value pair that requeires no advance schema.
    Massive throughput capabilities.
    PB size potential.
    Granular API access.


=> Knowledge check

1 - What are the scenarios in which you should use Amazon Relational Database Service (Amazon RDS)? (Select TWO.)

a) Running a serverless database

b) Using SQL to organize data

c) Storing data in a key-value database

d) Scaling up to 10 trillion requests per day

e) Storing data in an Amazon Aurora database

The two correct response options are:

Using SQL to organize data
Storing data in an Amazon Aurora database
The other three response options are scenarios in which you should use Amazon DynamoDB


=> Amazon Redshift
Amazon Redshift is a data warehousing service that you can use for big data analytics.
It offers the ability to collect data from many sources and helps you to understand relationships and trends across your data.


=> AWS Database Migration Service (AWS DMS)
AWS Database Migration Service (AWS DMS) enables you to migrate relational databases, nonrelational databases, and other types of data stores.
With AWS DMS, you move data between a source database and a target database. The source and target databases can be of the same type or different types. 
During the migration, your source database remains operational, reducing downtime for any applications that rely on the database. 
For example, suppose that you have a MySQL database that is stored on premises in an Amazon EC2 instance or in Amazon RDS. 
Consider the MySQL database to be your source database. Using AWS DMS, you could migrate your data to a target database, such as an Amazon Aurora database.

Other usecases:
    Development and test database migrations - Enabling developers to test applications against production data without affecting production users.
    Database consolidation - Combining several databases into a single database.
    Continuous replication - Sending ongoing copies of your data to other target sources instead of doing a one-time migration.


=> Additional Database Services
- Amazon DocumentDB
    Amazon DocumentDB is a document database service that supports MongoDB workloads. (MongoDB is a document database program.)
- Amazon Neptune
    Amazon Neptune is a graph database service. 
    You can use Amazon Neptune to build and run applications that work with highly connected datasets, 
    such as recommendation engines, fraud detection, and knowledge graphs.
- Amazon Quantum Ledger Database (Amazon QLDB)
    Amazon Quantum Ledger Database (Amazon QLDB) is a ledger database service.
    You can use Amazon QLDB to review a complete history of all the changes that have been made to your application data.
- Amazon Managed Blockchain
    Amazon Managed Blockchain is a service that you can use to create and manage blockchain networks with open-source frameworks. 
    Blockchain is a distributed ledger system that lets multiple parties run transactions and share data without a central authority.
- Amazon ElastiCache
    Amazon ElastiCache is a service that adds caching layers on top of your databases to help improve the read times of common requests. 
    It supports two types of data stores: Redis and Memcached.
- Amazon DynamoDB Accelerator
    Amazon DynamoDB Accelerator (DAX) is an in-memory cache for DynamoDB. 
    It helps improve response times from single-digit milliseconds to microseconds.


=> QUIZ

1 - Which Amazon S3 storage classes are optimized for archival data? (Select TWO.)

a) S3 Standard

b) S3 Glacier

c) S3 Intelligent-Tiering

d) S3 Standard-IA

e) S3 Glacier Deep Archive

The correct two response options are:

S3 Glacier
S3 Glacier Deep Archive

Objects stored in the S3 Glacier storage class can be retrieved within a few minutes to a few hours. 
By comparison, objects that are stored in the S3 Glacier Deep Archive storage class can be retrieved within 12 hours.

 
The other response options are incorrect because:

S3 Standard is a storage class that is ideal for frequently accessed data, not archival data.
S3 Intelligent-Tiering monitors access patterns of objects and automatically moves them between the S3 Standard and S3 Standard-IA storage classes. 
It is not designed for archival data.
S3 Standard-IA is ideal for data that is infrequently accessed but requires high availability when needed.


2 - Which statement or statements are TRUE about Amazon EBS volumes and Amazon EFS file systems?

a) EBS volumes store data within a single Availability Zone. Amazon EFS file systems store data across multiple Availability Zones.

b) EBS volumes store data across multiple Availability Zones. Amazon EFS file systems store data within a single Availability Zone.

c) EBS volumes and Amazon EFS file systems both store data within a single Availability Zone.

d) EBS volumes and Amazon EFS file systems both store data across multiple Availability Zones.

The correct response option is: EBS volumes store data within a single Availability Zone. Amazon EFS file systems store data across multiple Availability Zones.

 
An EBS volume must be located in the same Availability Zone as the Amazon EC2 instance to which it is attached.
Data in an Amazon EFS file system can be accessed concurrently from all the Availability Zones in the Region where the file system is located.


3 - You want to store data in an object storage service. Which AWS service is best for this type of storage?

a) Amazon Managed Blockchain

b) Amazon Elastic File System (Amazon EFS)

c) Amazon Elastic Block Store (Amazon EBS)

d) Amazon Simple Storage Service (Amazon S3)

The correct response option is Amazon Simple Storage Service (Amazon S3).

 
The other response options are incorrect because:

Amazon Managed Blockchain is a service that you can use to create and manage blockchain networks with open-source frameworks.
Blockchain is a distributed ledger system that lets multiple parties run transactions and share data without a central authority.
Amazon Elastic File System (Amazon EFS) is a scalable file system used with AWS Cloud services and on-premises resources. It does not store data as object storage.
Amazon Elastic Block Store (Amazon EBS) is a service that provides block-level storage volumes that you can use with Amazon EC2 instances.


4 - Which statement best describes Amazon DynamoDB?

a) A service that enables you to run relational databases in the AWS Cloud

b) A serverless key-value database service

c) A service that you can use to migrate relational databases, nonrelational databases, and other types of data stores

d) An enterprise-class relational database

The correct response option is A serverless key-value database service.

Amazon DynamoDB is a key-value database service. It is serverless, which means that you do not have to provision, patch, or manage servers.


The other response options are incorrect because:

A service that enables you to run relational databases in the AWS Cloud describes Amazon Relational Database Service (Amazon RDS).
A service that you can use to migrate relational databases, nonrelational databases, and other types of data stores describes AWS Database Migration Service (AWS DMS).
An enterprise-class relational database describes Amazon Aurora.


5 - Which service is used to query and analyze data across a data warehouse?

a) Amazon Redshift

b) Amazon Neptune

c) Amazon DocumentDB

d) Amazon ElastiCache

The correct response option is Amazon Redshift.

Amazon Redshift is a data warehousing service that you can use for big data analytics. 
Use Amazon Redshift to collect data from many sources and help you understand relationships and trends across your data.

 
The other response options are incorrect because:

Amazon Neptune is a graph database service. You can use Amazon Neptune to build and 
run applications that work with highly connected datasets, such as recommendation engines, fraud detection, and knowledge graphs.
Amazon DocumentDB is a document database service that supports MongoDB workloads.
Amazon ElastiCache is a service that adds caching layers on top of your databases to help improve the read times of common requests.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------

MODULE 6
Explain the benefits of the shared responsibility model.
Describe multi-factor authentication (MFA).
Differentiate between the AWS Identity and Access Management (IAM) security levels.
Explain the main benefits of AWS Organizations.
Describe security policies at a basic level.
Summarize the benefits of compliance with AWS.
Explain additional AWS security services at a basic level.


=> Shared Responsibility Model
The shared responsibility model divides into customer responsibilities (commonly referred to as “security in the cloud”) 
and AWS responsibilities (commonly referred to as “security of the cloud”).

- Customers: Security in the cloud
Customers are responsible for the security of everything that they create and put in the AWS Cloud.

- AWS: Security of the cloud
AWS is responsible for security of the cloud.
    Physical security of data centers
    Hardware and software infrastructure
    Network infrastructure
    Virtualization infrastructure


=> Knowledge check

1 - Which tasks are the responsibilities of customers? (Select TWO.)

a) Maintaining network infrastructure

b) Patching software on Amazon EC2 instances

c) Implementing physical security controls at data centers

d) Setting permissions for Amazon S3 objects

e) Maintaining servers that run Amazon EC2 instances

The correct two response options are:

Patching software on Amazon EC2 instances
Setting permissions for Amazon S3 objects
The other three response options are tasks that are the responsibility of AWS.


=> AWS Identity and Access Management (IAM)
    Users
    Groups
    Roles
    Policies
    Identity federation
    Multi-factor authentication (MFA)
It enables you to manage access to AWS services and resources securely.

IAM gives you the flexibility to configure access based on your company’s specific operational and security needs.
You do this by using a combination of IAM features, which are explored in detail in this lesson:
    IAM users, groups, and roles
    IAM policies
    Multi-factor authentication

- AWS account root user
When you first create an AWS account, you begin with an identity known as the root user.

Best practice:
Do not use the root user for everyday tasks.
Instead, use the root user to create your first IAM user and assign it permissions to create other users.
Then, continue to create other IAM users, and access those identities for performing regular tasks throughout AWS.
Only use the root user when you need to perform a limited number of tasks that are only available to the root user.
Examples of these tasks include changing your root user email address and changing your AWS support plan.

- IAM users
An IAM user is an identity that you create in AWS. It represents the person or application that interacts with AWS services and resources.
It consists of a name and credentials.
By default, when you create a new IAM user in AWS, it has no permissions associated with it.
To allow the IAM user to perform specific actions in AWS, such as launching an Amazon EC2 instance or creating an Amazon S3 bucket, 
you must grant the IAM user the necessary permissions.

Best practice:
We recommend that you create individual IAM users for each person who needs to access AWS.
Even if you have multiple employees who require the same level of access, you should create individual IAM users for each of them.
This provides additional security by allowing each IAM user to have a unique set of security credentials.


- IAM policies
An IAM policy is a document that allows or denies permissions to AWS services and resources.
IAM policies enable you to customize users’ levels of access to resources.
For example, you can allow users to access all of the Amazon S3 buckets within your AWS account, or only a specific bucket.

Best practice:
Follow the security principle of least privilege when granting permissions.
By following this principle, you help to prevent users or roles from having more permissions than needed to perform their tasks.

- IAM groups
It is a collection of IAM users. When you assign an IAM policy to a group, all users in the group are granted permissions specified by the policy.

- IAM roles
It is an identity that you can assume to gain temporary access to permissions.
Before an IAM user, application, or service can assume an IAM role, they must be granted permissions to switch to the role.
When someone assumes an IAM role, they abandon all previous permissions that they had under a previous role and assume the permissions of the new role.

Best practice:
IAM roles are ideal for situations in which access to services or resources needs to be granted temporarily, instead of long-term.

- Multi-factor authentication
It provides an extra layer of security for your AWS account, such as a random code sent to your phone.


=> AWS Organizations
    Centralized management
    Consolidated billing
    Hierarchical groupings of accounts
    AWS service API actions access control
You can use AWS Organizations to consolidate and manage multiple AWS accounts within a central location.
When you create an organization, AWS Organizations automatically creates a root, which is the parent container for all the accounts in your organization.
You can centrally control permissions for the accounts in your organization by using service control policies (SCPs).
SCPs enable you to place restrictions on the AWS services, resources, and individual API actions that users and roles in each account can access.

- Organizational units
In AWS Organizations, you can group accounts into organizational units (OUs) to make it easier to manage accounts with similar business or security requirements.
When you apply a policy to an OU, all the accounts in the OU automatically inherit the permissions specified in the policy.
By organizing separate accounts into OUs, you can more easily isolate workloads or applications that have specific security requirements.
For instance, if your company has accounts that can access only the AWS services that meet certain regulatory requirements, you can put these accounts into one OU.
Then, you can attach a policy to the OU that blocks access to all other AWS services that do not meet the regulatory requirements.


=> Knowledge check

1 - You are configuring service control policies (SCPs) in AWS Organizations. Which identities and resources can SCPs be applied to? (Select TWO.)

a) IAM users

b) IAM groups

c) An individual member account

d) IAM roles

e) An organizational unit (OU)

The correct two response options are:

An individual member account
An organizational unit (OU)
In AWS Organizations, you can apply service control policies (SCPs) to the organization root, an individual member account, 
or an OU. An SCP affects all IAM users, groups, and roles within an account, including the AWS account root user.


You can apply IAM policies to IAM users, groups, or roles. You cannot apply an IAM policy to the AWS account root user.


=> AWS Artifact
It is a service that provides on-demand access to AWS security and compliance reports and select online agreements.
AWS Artifact consists of two main sections: AWS Artifact Agreements and AWS Artifact Reports.

- AWS Artifact Agreements
In AWS Artifact Agreements, you can review, accept, and manage agreements for an individual account and for all your accounts in AWS Organizations.
Different types of agreements are offered to address the needs of customers who are subject to specific regulations,
such as the Health Insurance Portability and Accountability Act (HIPAA).

- AWS Artifact Reports
It provide compliance reports from third-party auditors. These auditors have tested and verified that AWS is compliant with a variety of global, 
regional, and industry-specific security standards and regulations. AWS Artifact Reports remains up to date with the latest reports released.
You can provide the AWS audit artifacts to your auditors or regulators as evidence of AWS security controls. 


=> Customer Compliance Center
The Customer Compliance Center contains resources to help you learn more about AWS compliance. 
In the Customer Compliance Center, you can read customer compliance stories to discover how companies in 
regulated industries have solved various compliance, governance, and audit challenges.
You can also access compliance whitepapers and documentation on topics such as:
    AWS answers to key compliance questions
    An overview of AWS risk and compliance
    An auditing security checklist
Additionally, the Customer Compliance Center includes an auditor learning path. This learning path is designed for individuals in auditing, 
compliance, and legal roles who want to learn more about how their internal operations can demonstrate compliance using the AWS Cloud.


=> Knowledge check

1 - Which tasks can you complete in AWS Artifact? (Select TWO.)

a) Access AWS compliance reports on-demand.

b) Consolidate and manage multiple AWS accounts within a central location.

c) Create users to enable people and applications to interact with AWS services and resources.

d) Set permissions for accounts by configuring service control policies (SCPs).

e) Review, accept, and manage agreements with AWS.

The correct two response options are:

Access AWS compliance reports on-demand.
Review, accept, and manage agreements with AWS.
The other response options are incorrect because:

Consolidate and manage multiple AWS accounts within a central location- This task can be completed in AWS Organizations.
Create users to enable people and applications to interact with AWS services and resources- This task can be completed in AWS Identity and Access Management (IAM).
Set permissions for accounts by configuring service control policies (SCPs)- This task can be completed in AWS Organizations.


=> AWS Shield
AWS Shield is a service that protects applications against DDoS attacks. AWS Shield provides two levels of protection: Standard and Advanced.

- AWS Shield Standard
AWS Shield Standard automatically protects all AWS customers at no cost.
It protects your AWS resources from the most common, frequently occurring types of DDoS attacks. 
As network traffic comes into your applications, AWS Shield Standard uses a variety of analysis 
techniques to detect malicious traffic in real time and automatically mitigates it. 

- AWS Shield Advanced
AWS Shield Advanced is a paid service that provides detailed attack diagnostics and the ability to detect and mitigate sophisticated DDoS attacks. 
It also integrates with other services such as Amazon CloudFront, Amazon Route 53, and Elastic Load Balancing.
Additionally, you can integrate AWS Shield with AWS WAF by writing custom rules to mitigate complex DDoS attacks.


=> AWS Key Management Service (AWS KMS)
It enables you to perform encryption operations through the use of cryptographic keys.
A cryptographic key is a random string of digits used for locking (encrypting) and unlocking (decrypting) data.
You can use AWS KMS to create, manage, and use cryptographic keys. You can also control the use of keys across a wide range of services and in your applications.
With AWS KMS, you can choose the specific levels of access control that you need for your keys. For example, 
you can specify which IAM users and roles are able to manage keys. 
Alternatively, you can temporarily disable keys so that they are no longer in use by anyone.
Your keys never leave AWS KMS, and you are always in control of them.


=> AWS WAF
AWS WAF is a web application firewall that lets you monitor network requests that come into your web applications. 
AWS WAF works together with Amazon CloudFront and an Application Load Balancer.
Recall the network access control lists that you learned about in an earlier module.
AWS WAF works in a similar way to block or allow traffic. However, it does this by using a web access control list (ACL) to protect your AWS resources. 


=> Amazon Inspector
Amazon Inspector helps to improve the security and compliance of applications by running automated security assessments.
It checks applications for security vulnerabilities and deviations from security best practices, 
such as open access to Amazon EC2 instances and installations of vulnerable software versions. 

After Amazon Inspector has performed an assessment, it provides you with a list of security findings.
The list prioritizes by severity level, including a detailed description of each security issue and a recommendation for how to fix it.
However, AWS does not guarantee that following the provided recommendations resolves every potential security issue.
Under the shared responsibility model, customers are responsible for the security of their applications, processes, and tools that run on AWS services.


=> Amazon GuardDuty
Amazon GuardDuty is a service that provides intelligent threat detection for your AWS infrastructure and resources.
It identifies threats by continuously monitoring the network activity and account behavior within your AWS environment.

After you have enabled GuardDuty for your AWS account, GuardDuty begins monitoring your network and account activity. 
You do not have to deploy or manage any additional security software. GuardDuty then continuously analyzes data from multiple AWS sources, 
including VPC Flow Logs and DNS logs. 

If GuardDuty detects any threats, you can review detailed findings about them from the AWS Management Console. 
Findings include recommended steps for remediation. You can also configure AWS Lambda functions to take remediation steps automatically 
in response to GuardDuty’s security findings.


=> QUIZ

1 - Which statement best describes an IAM policy?

a) An authentication process that provides an extra layer of protection for your AWS account

b) A document that grants or denies permissions to AWS services and resources

c) An identity that you can assume to gain temporary access to permissions

d) The identity that is established when you first create an AWS account

The correct response option is: A document that grants or denies permissions to AWS services and resources.

IAM policies provide you with the flexibility to customize users’ levels of access to resources.
For instance, you can allow users to access all the Amazon S3 buckets in your AWS account or only a specific bucket.


The other response options are incorrect because:

Multi-factor authentication (MFA) is an authentication process that provides an extra layer of protection for your AWS account.
An IAM role is an identity that you can assume to gain temporary access to permissions.
The root user identity is the identity that is established when you first create an AWS account.


2 - An employee requires temporary access to create several Amazon S3 buckets. Which option would be the best choice for this task?

a) AWS account root user

b) IAM group

c) IAM role

d) Service control policy (SCP)

The correct answer is IAM role.

An IAM role is an identity that you can assume to gain temporary access to permissions.
When someone assumes an IAM role, they abandon all permissions that they had under a previous role and assume the permissions of the new role.
IAM roles are ideal for situations in which access to services or resources needs to be granted temporarily instead of long-term.

 
The other response options are incorrect because:

The AWS account root user is established when you first create an AWS account. As a best practice, do not use the root user for everyday tasks.
Although you can attach IAM policies to an IAM group, this would not be the best choice for this scenario because the employee only needs 
to be granted temporary permissions.
Service control policies (SCPs) enable you to centrally control permissions for the accounts in your organization.
An SCP is not the best choice for granting temporary permissions to an individual employee.


3 - Which statement best describes the principle of least privilege?

a) Adding an IAM user into at least one IAM group

b) Checking a packet’s permissions against an access control list

c) Granting only the permissions that are needed to perform specific tasks

d) Performing a denial of service attack that originates from at least one device

The correct response option is: Granting only the permissions that are needed to perform specific job tasks.

When you grant permissions by following the principle of least privilege, you prevent users or roles from having more permissions than needed 
to perform specific job tasks. For example, cashiers in the coffee shop should be given access to the cash register system. 
As a best practice, grant IAM users and roles a minimum set of permissions and then grant additional permissions as needed.


4 - Which service helps protect your applications against distributed denial-of-service (DDoS) attacks?

a) Amazon GuardDuty

b) Amazon Inspector

c) AWS Artifact

d) AWS Shield

The correct response option is AWS Shield.

As network traffic comes into your applications, AWS Shield uses a variety of analysis techniques 
to detect potential DDoS attacks in real time and automatically mitigates them.


The other response options are incorrect because:
Amazon GuardDuty is a service that provides intelligent threat detection for your AWS infrastructure and resources.
It identifies threats by continuously monitoring the network activity and account behavior within your AWS environment.
Amazon Inspector checks applications for security vulnerabilities and deviations from security best practices, 
such as open access to Amazon EC2 instances and installations of vulnerable software versions.
AWS Artifact is a service that provides on-demand access to AWS security and compliance reports and select online agreements.


5 - Which task can AWS Key Management Service (AWS KMS) perform?

a) Configure multi-factor authentication (MFA).

b) Update the AWS account root user password.

c) Create cryptographic keys.

d) Assign permissions to users and groups.

The correct response option is: Create cryptographic keys.

AWS Key Management Service (AWS KMS) enables you to perform encryption operations through the use of cryptographic keys.
A cryptographic key is a random string of digits used for locking (encrypting) and unlocking (decrypting) data.
You can use AWS KMS to create, manage, and use cryptographic keys. You can also control the use of keys across a wide range of services and in your applications.

 
The other response options are incorrect because:

You can configure multi-factor authentication (MFA) in AWS Identity and Access Management (IAM).
You can update the AWS account root user password in the AWS Management Console.
You can assign permissions to users and groups in AWS Identity and Access Management (IAM).

---------------------------------------------------------------------------------------------------------------------------------------------------------------------

MODULE 7
Summarize approaches to monitoring your AWS environment.
Describe the benefits of Amazon CloudWatch.
Describe the benefits of AWS CloudTrail.
Describe the benefits of AWS Trusted Advisor.

=> Amazon CloudWatch
Amazon CloudWatch is a web service that enables you to monitor and manage various metrics and configure alarm actions based on data from those metrics.
CloudWatch uses metrics to represent the data points for your resources. AWS services send metrics to CloudWatch.
CloudWatch then uses these metrics to create graphs automatically that show how performance has changed over time.
    Access all your metrics from a central location
    Gain visibility into your applications, infrastructure and services
    Reduce MTTR (mean time to repair)
    Improve TCO (total cost of ownership)
    Drive insights to optimize applications and operational resources

- CloudWatch alarms
With CloudWatch, you can create alarms that automatically perform actions if the value of your metric has gone above or below a predefined threshold.

- CloudWatch dashboard
The CloudWatch dashboard feature enables you to access all the metrics for your resources from a single location.
For example, you can use a CloudWatch dashboard to monitor the CPU utilization of an Amazon EC2 instance, the total number of requests made to an 
Amazon S3 bucket, and more. You can even customize separate dashboards for different business purposes, applications, or resources.


=> AWS CloudTrail
AWS CloudTrail records API calls for your account. The recorded information includes the identity of the API caller, 
the time of the API call, the source IP address of the API caller, and more. You can think of CloudTrail as a “trail” of 
breadcrumbs (or a log of actions) that someone has left behind them.
Recall that you can use API calls to provision, manage, and configure your AWS resources. With CloudTrail, you can view a complete history of user 
activity and API calls for your applications and resources. 
Events are typically updated in CloudTrail within 15 minutes after an API call. 
You can filter events by specifying the time and date that an API call occurred, the user who requested the action, 
the type of resource that was involved in the API call, and more.

- CloudTrail Insights
Within CloudTrail, you can also enable CloudTrail Insights.
This optional feature allows CloudTrail to automatically detect unusual API activities in your AWS account. 
For example, CloudTrail Insights might detect that a higher number of Amazon EC2 instances than usual have recently launched in your account.
You can then review the full event details to determine which actions you need to take next.


=> Knowledge check

1 - Which tasks can you perform using AWS CloudTrail? (Select TWO.)

a) Monitor your AWS infrastructure and resources in real time

b) Track user activities and API requests throughout your AWS infrastructure

c) View metrics and graphs to monitor the performance of resources

d) Filter logs to assist with operational analysis and troubleshooting

e) Configure automatic actions and alerts in response to metrics

The correct two response options are:

Track user activities and API requests throughout your AWS infrastructure
Filter logs to assist with operational analysis and troubleshooting
The other response options are tasks that you can perform in Amazon CloudWatch.


=> AWS Trusted Advisor
AWS Trusted Advisor is a web service that inspects your AWS environment and provides real-time recommendations in accordance with AWS best practices.
Trusted Advisor compares its findings to AWS best practices in five categories: cost optimization, performance, security, fault tolerance, and service limits.
For the checks in each category, Trusted Advisor offers a list of recommended actions and additional resources to learn more about AWS best practices. 

The guidance provided by AWS Trusted Advisor can benefit your company at all stages of deployment.
For example, you can use AWS Trusted Advisor to assist you while you are creating new workflows and developing new applications.
Or you can use it while you are making ongoing improvements to existing applications and resources.

    Cost optimization
    Performance
    Security
    Fault tolerance
    Service limits

- AWS Trusted Advisor dashboard
When you access the Trusted Advisor dashboard on the AWS Management Console, you can review completed checks for 
cost optimization, performance, security, fault tolerance, and service limits.

For each category:
    The green check indicates the number of items for which it detected no problems.
    The orange triangle represents the number of recommended investigations.
    The red circle represents the number of recommended actions.


=> QUIZ

1 - Which actions can you perform using Amazon CloudWatch? (Select TWO.)

a) Monitor your resources’ utilization and performance

b) Receive real-time guidance for improving your AWS environment

c) Compare your infrastructure to AWS best practices in five categories

d) Access metrics from a single dashboard

e) Automatically detect unusual account activity

The two correct response options are:

Monitor your resources’ utilization and performance
Access metrics from a single dashboard


The other response options are incorrect because:

Receiving real-time recommendations for improving your AWS environment can be performed by AWS Trusted Advisor.
Comparing your infrastructure to AWS best practices in five categories can be performed by AWS Trusted Advisor.
Automatically detecting unusual account activity can be performed by AWS CloudTrail.


2 - Which service enables you to review the security of your Amazon S3 buckets by checking for open access permissions?

a) Amazon CloudWatch

b) AWS CloudTrail

c) AWS Trusted Advisor

d) Amazon GuardDuty

The correct response option is AWS Trusted Advisor.

AWS Trusted Advisor is a web service that inspects your AWS environment and provides real-time recommendations in accordance with AWS best practices.
The inspection includes security checks, such as Amazon S3 buckets with open access permissions.


The other response options are incorrect because:

Amazon CloudWatch is a web service that enables you to monitor and manage various metrics for the resources that run your applications.
AWS CloudTrail is a web service that enables you to review details for user activities and API calls that have occurred within your AWS environment.
Amazon GuardDuty is a service that provides intelligent threat detection for your AWS environment and resources.
It identifies threats by continuously monitoring the network activity and account behavior within your AWS environment.


3 - Which categories are included in the AWS Trusted Advisor dashboard? (Select TWO.)

a) Reliability

b) Performance

c) Scalability

d) Elasticity

e) Fault tolerance

The two correct response options are:

Performance
Fault tolerance


AWS Trusted Advisor continuously inspects your AWS environment and provides best practice recommendations across five categories: 
cost optimization, performance, security, fault tolerance, and service limits.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------

MODULE 8
Describe AWS pricing and support models.
Describe the AWS Free Tier.
Describe key benefits of AWS Organizations and consolidated billing.
Explain the benefits of AWS Budgets.
Explain the benefits of AWS Cost Explorer.
Explain the primary benefits of the AWS Pricing Calculator.
Distinguish between the various AWS Support Plans.
Describe the benefits of AWS Marketplace.

=> AWS Free Tier
The AWS Free Tier enables you to begin using certain services without having to worry about incurring costs for the specified period. 
Three types of offers are available: 
    Always Free
    12 Months Free
    Trials

For each free tier offer, make sure to review the specific details about exactly which resource types are included.

- Always Free
These offers do not expire and are available to all AWS customers.
For example, AWS Lambda allows 1 million free requests and up to 3.2 million seconds of compute time per month.
Amazon DynamoDB allows 25 GB of free storage per month.

- 12 Months Free
These offers are free for 12 months following your initial sign-up date to AWS.
Examples include specific amounts of Amazon S3 Standard Storage, thresholds for monthly hours of Amazon EC2 compute time,
and amounts of Amazon CloudFront data transfer out.

- Trials
Short-term free trial offers start from the date you activate a particular service.
The length of each trial might vary by number of days or the amount of usage in the service.
For example, Amazon Inspector offers a 90-day free trial.
Amazon Lightsail (a service that enables you to run virtual private servers) offers 750 free hours of usage over a 30-day period.


=> Knowledge check

1 - The AWS Free Tier includes offers that are available to new AWS customers for a certain period of 
time following their AWS sign-up date. What is the duration of this period?

a) 3 months

b) 6 months

c) 9 months

d) 12 months

The correct response option is 12 months.

The AWS Free Tier consists of three types of offers that allow customers to use AWS services without incurring costs: 
Always free, 12 months free, and Trials.


For 12 months after you first sign up for an AWS account, you can take advantage of offers in the 12 Months Free category. 
Examples of offers in this category include specific amounts of Amazon S3 Standard Storage, 
thresholds for monthly hours of Amazon EC2 compute time, and amounts of Amazon CloudFront data transfer out.


=> How AWS pricing works
AWS offers a range of cloud computing services with pay-as-you-go pricing. 

- Pay for what you use.
For each service, you pay for exactly the amount of resources that you actually use, without requiring long-term contracts or complex licensing. 

- Pay less when you reserve.
Some services offer reservation options that provide a significant discount compared to On-Demand Instance pricing.
For example, suppose that your company is using Amazon EC2 instances for a workload that needs to run continuously.
You might choose to run this workload on Amazon EC2 Instance Savings Plans,
because the plan allows you to save up to 72% over the equivalent On-Demand Instance capacity.

- Pay less with volume-based discounts when you use more.
Some services offer tiered pricing, so the per-unit cost is incrementally lower with increased usage.
For example, the more Amazon S3 storage space you use, the less you pay for it per GB.


=> AWS Pricing Calculator
The AWS Pricing Calculator lets you explore AWS services and create an estimate for the cost of your use cases on AWS.
You can organize your AWS estimates by groups that you define. A group can reflect how your company is organized, such as providing estimates by cost center.
When you have created an estimate, you can save it and generate a link to share it with others.


=> AWS pricing examples
This section presents a few examples of pricing in AWS services. 

- AWS Lambda
For AWS Lambda, you are charged based on the number of requests for your functions and the time that it takes for them to run.
AWS Lambda allows 1 million free requests and up to 3.2 million seconds of compute time per month.
You can save on AWS Lambda costs by signing up for Compute Savings Plans. 
Compute Savings Plans offer lower compute costs in exchange for committing to a consistent amount of usage over a 1-year or 3-year term.
This is an example of paying less when you reserve.

If you have used AWS Lambda in multiple AWS Regions, you can view the itemized charges by Region on your bill. 
In this example, all the AWS Lambda usage occurred in the Northern Virginia Region.
The bill lists separate charges for the number of requests for functions and their duration. 
Both the number of requests and the total duration of requests in this example are under the thresholds in the AWS Free Tier,
so the account owner would not have to pay for any AWS Lambda usage in this month.

- Amazon EC2

With Amazon EC2, you pay for only the compute time that you use while your instances are running.
For some workloads, you can significantly reduce Amazon EC2 costs by using Spot Instances.
For example, suppose that you are running a batch processing job that is able to withstand interruptions.
Using a Spot Instance would provide you with up to 90% cost savings while still meeting the availability requirements of your workload.
You can find additional cost savings for Amazon EC2 by considering Savings Plans and Reserved Instances.

The service charges in this example include details for the following items:
Each Amazon EC2 instance type that has been used
The amount of Amazon EBS storage space that has been provisioned
The length of time that Elastic Load Balancing has been used

In this example, all the usage amounts are under the thresholds in the AWS Free Tier, 
so the account owner would not have to pay for any Amazon EC2 usage in this month.


=> AWS Billing & Cost Management dashboard
Use the AWS Billing & Cost Management dashboard to pay your AWS bill, monitor your usage, and analyze and control your costs.

    Compare your current month-to-date balance with the previous month, and get a forecast of the next month based on current usage.
    View month-to-date spend by service.
    View Free Tier usage by service.
    Access Cost Explorer and create budgets.
    Purchase and manage Savings Plans.
    Publish AWS Cost and Usage Reports.


=> Consolidated billing
The consolidated billing feature of AWS Organizations enables you to receive a single bill for all AWS accounts in your organization.
By consolidating, you can easily track the combined costs of all the linked accounts in your organization.
The default maximum number of accounts allowed for an organization is 4, but you can contact AWS Support to increase your quota, if needed.

On your monthly bill, you can review itemized charges incurred by each account. This enables you to have greater transparency into your organization’s 
accounts while still maintaining the convenience of receiving a single monthly bill.

Another benefit of consolidated billing is the ability to share bulk discount pricing, Savings Plans, and Reserved Instances across the 
accounts in your organization. For instance, one account might not have enough monthly usage to qualify for discount pricing. 
However, when multiple accounts are combined, their aggregated usage may result in a benefit that applies across all accounts in the organization.

    Simplifies billing process
    Share savings across accounts
    Free feature


=> AWS Budgets
In AWS Budgets, you can create budgets to plan your service usage, service costs, and instance reservations.
The information in AWS Budgets updates three times a day. This helps you to accurately determine how close your usage is to your budgeted amounts 
or to the AWS Free Tier limits.
In AWS Budgets, you can also set custom alerts when your usage exceeds (or is forecasted to exceed) the budgeted amount.

Example: AWS Budgets
Suppose that you have set a budget for Amazon EC2. You want to ensure that your company’s usage of Amazon EC2 does not exceed $200 for the month.
In AWS Budgets, you could set a custom budget to notify you when your usage has reached half of this amount ($100).
This setting would allow you to receive an alert and decide how you would like to proceed with your continued use of Amazon EC2.


=> AWS Cost Explorer
AWS Cost Explorer is a tool that enables you to visualize, understand, and manage your AWS costs and usage over time.
AWS Cost Explorer includes a default report of the costs and usage for your top five cost-accruing AWS services.
You can apply custom filters and groups to analyze your data. For example, you can view resource usage at the hourly level.


=> AWS Support
AWS offers four different Support plans to help you troubleshoot issues, lower costs, and efficiently use AWS services. 
You can choose from the following Support plans to meet your company’s needs:
    Basic
    Developer
    Business
    Enterprise

- Basic
    24/7 customer service
    Documentation
    Whitepapers
    Support forums
    AWS Trusted Advisor
    AWS Personal Health Dashboard

- Developer
    Basic + email access to customer support

- Business
    Basic and Developer
    AWS Trusted Advisor provides full set of best practices checks
    Direct phone access to cloud support engineers (4h SLA to production system is impaired and 1h SLA if system is down)
    Infrastructure event management

- Enterprise
    Bsic, Developer and Business
    15 minutes SLA for business critical workload
    Technical Account Manager (TAM)

=> Technical Account Manager (TAM)
The Enterprise Support plan includes access to a Technical Account Manager (TAM).
If your company has an Enterprise Support plan, the TAM is your primary point of contact at AWS. 
They provide guidance, architectural reviews, and ongoing communication with your company as you plan, deploy, and optimize your applications. 

Your TAM provides expertise across the full range of AWS services. They help you design solutions that efficiently use multiple services 
together through an integrated approach.

For example, suppose that you are interested in developing an application that uses several AWS services together.
Your TAM could provide insights into how to best use the services together.
They achieve this, while aligning with the specific needs that your company is hoping to address through the new application.

- Five pillars of the Well-Architected Framework
    Operational Excellence
    Security
    Reliability
    Performance Efficiency
    Cost Optimization


=> Knowledge check

1 - Which Support plan includes all AWS Trusted Advisor checks at the lowest cost?

a) Basic

b) Developer

c) Business

d) Enterprise

The correct response option is Business.

Only the Business and Enterprise Support plans include all AWS Trusted Advisor checks. Of these two Support plans, the Business Support plan has a lower cost.


=> AWS Marketplace
    Custom terms and pricing
    A private marketplace
    Integration into your procurement systems
    Cost management tools

AWS Marketplace is a digital catalog that includes thousands of software listings from independent software vendors.
You can use AWS Marketplace to find, test, and buy software that runs on AWS. 
For each listing in AWS Marketplace, you can access detailed information on pricing options, available support, and reviews from other AWS customers.

You can also explore software solutions by industry and use case. For example, suppose that your company is in the healthcare industry.
In AWS Marketplace, you can review use cases that software helps you to address, such as implementing solutions to protect patient records or 
using machine learning models to analyze a patient’s medical history and predict possible health risks.

- AWS Marketplace categories
    Business Applications
    Data & Analytics
    DevOps
    Infrastructure Software
    Internet of Things (IoT)
    Machine Learning
    Migration
    Security


=> QUIZ

1 - Which action can you perform with consolidated billing?

a) Review how much cost your predicted AWS usage will incur by the end of the month.

b) Create an estimate for the cost of your use cases on AWS.

c) Combine usage across accounts to receive volume pricing discounts.

d) Visualize and manage your AWS costs and usage over time.

The correct response option is: Combine usage across accounts to receive volume pricing discounts.


The other response options are incorrect because:

Review how much cost your predicted AWS usage will incur by the end of the month - You can perform this action in AWS Budgets.
Create an estimate for the cost of your use cases on AWS - You can perform this action in AWS Pricing Calculator.
Visualize and manage your AWS costs and usage over time - You can perform this action in AWS Cost Explorer.


2 - Which pricing tool is used to visualize, understand, and manage your AWS costs and usage over time?

a) AWS Pricing Calculator

b) AWS Budgets

c) AWS Cost Explorer

d) AWS Free Tier

The correct response option is AWS Cost Explorer.

AWS Cost Explorer includes a default report of the costs and usage for your top five cost-accruing AWS services.
You can apply custom filters and groups to analyze your data. For example, you can view resource usage at the hourly level.

 
The other response options are incorrect because:

AWS Pricing Calculator enables you to create an estimate for the cost of your use cases on AWS.
AWS Budgets enables you to create budgets to plan your service usage, service costs, and instance reservations.
In AWS Budgets, you can also set custom alerts when your usage exceeds (or is forecasted to exceed) the budgeted amount.
The AWS Free Tier is a program that consists of three types of offers that allow customers to use AWS services without incurring costs: 
Always free, 12 months free, and Trials.


3 - Which pricing tool enables you to receive alerts when your service usage exceeds a threshold that you have defined?

a) Billing dashboard in the AWS Management Console

b) AWS Budgets

c) AWS Free Tier

d) AWS Cost Explorer

The correct response option is AWS Budgets.

In AWS Budgets, you can set custom alerts that will notify you when your service usage exceeds (or is forecasted to exceed) the amount that you have budgeted.
Your budget can be based on costs or usage. For example, you can set an alert that will notify you when you have 
incurred $100.00 of costs in Amazon EC2 or 500,000 requests in AWS Lambda.
 

The other response options are incorrect because:

From the billing dashboard in the AWS Management Console, you can view details on your AWS bill, such as service costs by Region, 
month to date spend, and more. However, you cannot set alerts from the billing dashboard.
The AWS Free Tier is a program that consists of three types of offers that allow customers to use AWS services without incurring costs: 
Always free, 12 months free, and Trials.
AWS Cost Explorer is a tool that enables you to visualize, understand, and manage your AWS costs and usage over time.


4 - Your company wants to receive support from an AWS Technical Account Manager (TAM). Which support plan should you choose?

a) Developer

b) Enterprise

c) Basic

d) Business

The correct response option is Enterprise.

A Technical Account Manager (TAM) is available only to AWS customers with an Enterprise Support plan.
A TAM provides guidance, architectural reviews, and ongoing communication with your company as you plan, deploy, and optimize your applications.


5 - Which service or resource is used to find third-party software that runs on AWS?

a) AWS Marketplace

b) AWS Free Tier

c) AWS Support

d) Billing dashboard in the AWS Management Console

The correct response option is AWS Marketplace.

AWS Marketplace is a digital catalog that includes thousands of software listings from independent software vendors.
You can use AWS Marketplace to find, test, and buy software that runs on AWS.
 

The other response options are incorrect because:

The AWS Free Tier consists of offers that allow customers to use AWS services without incurring costs. 
These offers are related to AWS services, not third-party software that can be used on AWS.

AWS Support is a resource that can answer questions about best practices, assist with troubleshooting issues, 
help you to identify ways to optimize your use of AWS services, and so on.

You can use the billing dashboard in the AWS Management Console to view details such as service costs by Region, 
the top services being used by your account, and forecasted billing costs. From the billing dashboard, you can also access 
other AWS billing tools, such as AWS Cost Explorer, AWS Budgets, and AWS Budgets Reports.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------

MODULE 9
Understand migration and innovation in the AWS Cloud.
Summarize the AWS Cloud Adoption Framework (AWS CAF). 
Summarize the six key factors of a cloud migration strategy.
Describe the benefits of AWS data migration solutions, such as AWS Snowcone, AWS Snowball, and AWS Snowmobile.
Summarize the broad scope of innovative solutions that AWS offers.

=> Six core perspectives of the Cloud Adoption Framework
    Business Perspective
    People Perspective
    Governance Perspective
    Platform Perspective
    Security Perspective
    Operations Perspective

At the highest level, the AWS Cloud Adoption Framework (AWS CAF) organizes guidance into six areas of focus, called Perspectives. 
Each Perspective addresses distinct responsibilities. The planning process helps the right people across the organization prepare for the changes ahead.
In general, the Business, People, and Governance Perspectives focus on business capabilities, 
whereas the Platform, Security, and Operations Perspectives focus on technical capabilities.

- Business Perspective
The Business Perspective ensures that IT aligns with business needs and that IT investments link to key business results.
Use the Business Perspective to create a strong business case for cloud adoption and prioritize cloud adoption initiatives.
Ensure that your business strategies and goals align with your IT strategies and goals.
Common roles in the Business Perspective include: 
    Business managers
    Finance managers
    Budget owners
    Strategy stakeholders

- People Perspective
The People Perspective supports development of an organization-wide change management strategy for successful cloud adoption.
Use the People Perspective to evaluate organizational structures and roles, new skill and process requirements, and identify gaps.
This helps prioritize training, staffing, and organizational changes.
Common roles in the People Perspective include: 
    Human resources
    Staffing
    People managers

- Governance Perspective
The Governance Perspective focuses on the skills and processes to align IT strategy with business strategy.
This ensures that you maximize the business value and minimize risks.
Use the Governance Perspective to understand how to update the staff skills and processes necessary to ensure business governance in the cloud.
Manage and measure cloud investments to evaluate business outcomes.
Common roles in the Governance Perspective include: 
    Chief Information Officer (CIO)
    Program managers
    Enterprise architects
    Business analysts
    Portfolio managers

- Platform Perspective
The Platform Perspective includes principles and patterns for implementing new solutions on the cloud, and migrating on-premises workloads to the cloud.
Use a variety of architectural models to understand and communicate the structure of IT systems and their relationships.
Describe the architecture of the target state environment in detail.
Common roles in the Platform Perspective include:
    Chief Technology Officer (CTO)
    IT managers
    Solutions architects

- Security Perspective
The Security Perspective ensures that the organization meets security objectives for visibility, auditability, control, and agility. 
Use the AWS CAF to structure the selection and implementation of security controls that meet the organization’s needs.
Common roles in the Security Perspective include:
    Chief Information Security Officer (CISO)
    IT security managers
    IT security analysts

- Operations Perspective
The Operations Perspective helps you to enable, run, use, operate, and recover IT workloads to the level agreed upon with your business stakeholders.
Define how day-to-day, quarter-to-quarter, and year-to-year business is conducted. Align with and support the operations of the business.
The AWS CAF helps these stakeholders define current operating procedures and 
identify the process changes and training needed to implement successful cloud adoption.
Common roles in the Operations Perspective include:
    IT operations managers
    IT support managers


=> Knowledge check

1 - Which Perspective of the AWS Cloud Adoption Framework helps you design, implement, and optimize your 
AWS infrastructure based on your business goals and perspectives?

a) Business Perspective

b) Platform Perspective

c) Operations Perspective

d) People Perspective

The correct response option is Platform Perspective.
The Platform Perspective of the AWS Cloud Adoption Framework also includes principles for implementing 
new solutions and migrating on-premises workloads to the cloud.


The other response options are incorrect because:

The Business Perspective helps you to move from a model that separates business and IT strategies into a business model that integrates IT strategy.
The Operations Perspective focuses on operating and recovering IT workloads to meet the requirements of your business stakeholders.
The People Perspective helps Human Resources (HR) employees prepare their teams for cloud adoption by updating 
organizational processes and staff skills to include cloud-based competencies.


=> 6 strategies for migration
When migrating applications to the cloud, six of the most common migration strategies that you can implement are:
    Rehosting
    Replatforming
    Refactoring/re-architecting
    Repurchasing
    Retaining
    Retiring

- Rehosting
Rehosting also known as “lift-and-shift” involves moving applications without changes.
In the scenario of a large legacy migration, in which the company is looking to implement its migration and scale quickly 
to meet a business case, the majority of applications are rehosted.  

- Replatforming
Replatforming, also known as “lift, tinker, and shift,” involves making a few cloud optimizations to realize a tangible benefit.
Optimization is achieved without changing the core architecture of the application.

- Refactoring/re-architecting
Refactoring (also known as re-architecting) involves reimagining how an application is architected and developed by using cloud-native features.
Refactoring is driven by a strong business need to add features, scale, or performance that would otherwise be difficult to achieve in 
the application’s existing environment.

- Repurchasing
Repurchasing involves moving from a traditional license to a software-as-a-service model.
For example, a business might choose to implement the repurchasing strategy by migrating from a customer 
relationship management (CRM) system to Salesforce.com.

- Retaining
Retaining consists of keeping applications that are critical for the business in the source environment.
This might include applications that require major refactoring before they can be migrated, or, work that can be postponed until a later time.

- Retiring
Retiring is the process of removing applications that are no longer needed.


=> Knowledge check

1 - Which migration strategy involves moving to a different product?

a) Refactoring

b) Retiring

c) Replatforming

d) Repurchasing

The correct response option is Repurchasing.

Repurchasing involves replacing an existing application with a cloud-based version, such as software found in AWS Marketplace.


The other response options are incorrect because:

Refactoring involves changing how an application is architected and developed, typically by using cloud-native features.
Retiring involves removing an application that is no longer used or that can be turned off.
Replatforming involves selectively optimizing aspects of an application to achieve benefits in the cloud without changing 
the core architecture of the application. It is also known as “lift, tinker, and shift.”


=> AWS Snow Family members
The AWS Snow Family is a collection of physical devices that help to physically transport up to exabytes of data into and out of AWS.
AWS Snow Family is composed of AWS Snowcone, AWS Snowball, and AWS Snowmobile. 

These devices offer different capacity points, and most include built-in computing capabilities.
AWS owns and manages the Snow Family devices and integrates with AWS security, monitoring, storage management, and computing capabilities. 

- AWS Snowcone: 
Small, rugged, and secure edge computing and data transfer device.
It features 2 CPUs, 4 GB of memory, and 8 TB of usable storage.

- AWS Snowball
Snowball Edge Storage Optimized devices are well suited for large-scale data migrations and recurring transfer workflows, 
in addition to local computing with higher capacity needs. 
    Storage: 80 TB of hard disk drive (HDD) capacity for block volumes and Amazon S3 compatible object storage, 
    and 1 TB of SATA solid state drive (SSD) for block volumes. 
    Compute: 40 vCPUs, and 80 GiB of memory to support Amazon EC2 sbe1 instances (equivalent to C5).
Snowball Edge Compute Optimized provides powerful computing resources for use cases such as machine learning, 
full motion video analysis, analytics, and local computing stacks. 
    Storage: 42-TB usable HDD capacity for Amazon S3 compatible object storage or Amazon EBS compatible block volumes and 7.68 TB of 
    usable NVMe SSD capacity for Amazon EBS compatible block volumes. 
    Compute: 52 vCPUs, 208 GiB of memory, and an optional NVIDIA Tesla V100 GPU. Devices run Amazon EC2 sbe-c and sbe-g instances, 
    which are equivalent to C5, M5a, G3, and P3 instances.

- AWS Snowmobile
Exabyte-scale data transfer service used to move large amounts of data to AWS. 
You can transfer up to 100 petabytes of data per Snowmobile, a 45-foot long ruggedized shipping container, pulled by a semi trailer truck.


=> Knowledge check

1- What is the storage capacity of Snowball Edge Storage Optimized?

a) 40 TB

b) 60 TB

c) 80 TB

d) 100 TB

The correct response option is 80 TB.

Snowball Edge Storage Optimized is a device that enables you to transfer large amounts of data 
into and out of AWS. It provides 80 TB of usable HDD storage.


=> Innovate with AWS Services
When examining how to use AWS services, it is important to focus on the desired outcomes. 
You are properly equipped to drive innovation in the cloud if you can clearly articulate the following conditions:
    The current state
    The desired state
    The problems you are trying to solve
Consider some of the paths you might explore in the future as you continue on your cloud journey. 

- Serverless applications
With AWS, serverless refers to applications that don’t require you to provision, maintain, or administer servers.
You don’t need to worry about fault tolerance or availability. AWS handles these capabilities for you.

AWS Lambda is an example of a service that you can use to run serverless applications. 
If you design your architecture to trigger Lambda functions to run your code, you can bypass the need to manage a fleet of servers.
Building your architecture with serverless applications enables your developers to focus on their core product instead of managing and operating servers.

- Artificial intelligence
AWS offers a variety of services powered by artificial intelligence (AI). 
For example, you can perform the following tasks:
    Convert speech to text with Amazon Transcribe.
    Discover patterns in text with Amazon Comprehend.
    Identify potentially fraudulent online activities with Amazon Fraud Detector.
    Build voice and text chatbots with Amazon Lex.

- Machine learning
Traditional machine learning (ML) development is complex, expensive, time consuming, and error prone.
AWS offers Amazon SageMaker to remove the difficult work from the process and empower you to build, train, and deploy ML models quickly.
You can use ML to analyze data, solve complex problems, and predict outcomes before they happen.


=> Knowledge check

1 - Which service enables you to quickly build, train, and deploy machine learning models?

a) Amazon Textract

b) Amazon Lex

c) AWS DeepRacer

d) Amazon SageMaker

The correct response option is Amazon SageMaker.

With Amazon SageMaker, you can quickly and easily begin working on machine learning projects.
You do not need to follow the traditional process of manually bringing together separate tools and workflows.


The other response options are incorrect because:

Amazon Textract is a machine learning service that automatically extracts text and data from scanned documents.
Amazon Lex is a service that enables you to build conversational interfaces using voice and text.
AWS DeepRacer is an autonomous 1/18 scale race car that you can use to test reinforcement learning models.


=> QUIZ

1 - Which Perspective of the AWS Cloud Adoption Framework helps you structure the selection and implementation of permissions?

a) Governance Perspective

b) Security Perspective

c) Operations Perspective

d) Business Perspective

The correct response option is Security Perspective.

The Security Perspective of the AWS Cloud Adoption Framework also helps you to identify 
areas on non-compliance and plan ongoing security initiatives.


The other response options are incorrect because:

The Governance Perspective helps you to identify and implement best practices for IT governance and support business processes with technology.
The Operations Perspective focuses on operating and recovering IT workloads to meet the requirements of your business stakeholders.
The Business Perspective helps you to move from a model that separates business and IT strategies into a business model that integrates IT strategy.


2 - Which strategies are included in the six strategies for application migration? (Select TWO.)

a) Revisiting

b) Retaining

c) Remembering

d) Redeveloping

e) Rehosting

The two correct response options are:

Retaining
Rehosting
The application migration strategies are rehosting, replatforming, refactoring/re-architecting, repurchasing, retaining, and retiring.


3 - What is the storage capacity of AWS Snowmobile?

a) 40 PB

b) 60 PB

c) 80 PB

d) 100 PB

The correct response option is 100 PB.

AWS Snowmobile is a service that is used for transferring up to 100 PB of data to AWS. 
Each Snowmobile is a 45-foot long shipping container that is pulled by a semi trailer truck.


4 - Which statement best describes Amazon Lex?

a) A service that enables you to build conversational interfaces using voice and text

b) A machine learning service that automatically extracts text and data from scanned documents

c) A document database service that supports MongoDB workloads

d) A service that enables you to identify potentially fraudulent online activities

The correct response option is Amazon Lex. 

In Amazon Lex, you can quickly build, test, and deploy conversational chatbots to use in your applications.


The other response options are incorrect because:

A machine learning service that automatically extracts text and data from scanned document describes Amazon Textract.
A document database service that supports MongoDB workloads describes Amazon DocumentDB.
A service that enables you to identify potentially fraudulent online activities describes Amazon Fraud Detector.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------

MODULE 10
Summarize the five pillars of the Well-Architected Framework.  
Explain the six benefits of cloud computing.

=> The AWS Well-Architected Framework
    Operational excellence
    Security
    Reliability
    Performance efficiency
    Cost optimization

- Operational excellence
Operational excellence is the ability to run and monitor systems to deliver business value and to continually improve 
supporting processes and procedures.  
Design principles for operational excellence in the cloud include performing operations as code, annotating documentation, 
anticipating failure, and frequently making small, reversible changes.

- Security
The Security pillar is the ability to protect information, systems, and assets while delivering business value through risk assessments 
and mitigation strategies. 
When considering the security of your architecture, apply these best practices:
    Automate security best practices when possible.
    Apply security at all layers.
    Protect data in transit and at rest.

- Reliability
Reliability is the ability of a system to do the following:
    Recover from infrastructure or service disruptions
    Dynamically acquire computing resources to meet demand
    Mitigate disruptions such as misconfigurations or transient network issues
Reliability includes testing recovery procedures, scaling horizontally to increase 
aggregate system availability, and automatically recovering from failure.

- Performance efficiency
Performance efficiency is the ability to use computing resources efficiently to meet system requirements and to maintain that 
efficiency as demand changes and technologies evolve. 
Evaluating the performance efficiency of your architecture includes experimenting more often, using serverless architectures, 
and designing systems to be able to go global in minutes.

- Cost optimization
Cost optimization is the ability to run systems to deliver business value at the lowest price point. 
Cost optimization includes adopting a consumption model, analyzing and attributing expenditure, 
and using managed services to reduce the cost of ownership.


=> Knowledge check

1 - Which pillar of the AWS Well-Architected Framework focuses on the ability of a workload to consistently and correctly perform its intended functions?

a) Operational Excellence

b) Performance Efficiency

c) Security

d) Reliability

The correct response option is Reliability.


The other response options are incorrect because:

The Operational Excellence pillar includes the ability to run workloads effectively, gain insights into their operations, and continuously 
improve supporting processes to deliver business value.
The Performance Efficiency pillar focuses on using computing resources efficiently to meet system requirements, 
and to maintain that efficiency as demand changes and technologies evolve.
The Security pillar includes protecting data, systems, and assets, and using cloud technologies to improve the security of your workloads.


=> Advantages of cloud computing
Operating in the AWS Cloud offers many benefits over computing in on-premises or hybrid environments. 
In this section, you will learn about six advantages of cloud computing:
    Trade upfront expense for variable expense.
    Benefit from massive economies of scale.
    Stop guessing capacity.
    Increase speed and agility.
    Stop spending money running and maintaining data centers.
    Go global in minutes.

- Trade upfront expense for variable expense.
Upfront expenses include data centers, physical servers, and other resources that you would need to invest in before using computing resources. 
Instead of investing heavily in data centers and servers before you know how you’re 
going to use them, you can pay only when you consume computing resources.

- Benefit from massive economies of scale.
By using cloud computing, you can achieve a lower variable cost than you can get on your own.
Because usage from hundreds of thousands of customers aggregates in the cloud, providers such as AWS can achieve higher economies of scale. 
Economies of scale translate into lower pay-as-you-go prices.

- Stop guessing capacity.
With cloud computing, you don’t have to predict how much infrastructure capacity you will need before deploying an application. 
For example, you can launch Amazon Elastic Compute Cloud (Amazon EC2) instances when needed and pay only for the compute time you use.
Instead of paying for resources that are unused or dealing with limited capacity, you can access only the capacity that you need, 
and scale in or out in response to demand.

- Increase speed and agility.
The flexibility of cloud computing makes it easier for you to develop and deploy applications.
This flexibility also provides your development teams with more time to experiment and innovate.

- Stop spending money running and maintaining data centers.
Cloud computing in data centers often requires you to spend more money and time managing infrastructure and servers. 
A benefit of cloud computing is the ability to focus less on these tasks and more on your applications and customers.

- Go global in minutes.
The AWS Cloud global footprint enables you to quickly deploy applications to customers around the world, while providing them with low latency.


=> Knowledge check

1 - Which process is an example of benefiting from massive economies of scale?

a) Deploying an application in multiple Regions around the world

b) Receiving lower pay-as-you-go prices as the result of AWS customers’ aggregated usage of services

c) Paying for compute time as you use it instead of investing upfront costs in data centers

d) Scaling your infrastructure capacity in and out to meet demand

The correct response option is: Receiving lower pay-as-you-go prices as the result of AWS customers’ aggregated usage of services.

Because usage from hundreds of thousands of customers is aggregated in the cloud, providers such as AWS can achieve higher economies of scale. 
The economies of scale translate into lower pay-as-you-go prices. 


The other response options are incorrect because:

Deploying an application in multiple Regions around the world: This process is an example of Go global in minutes.
Paying for compute time as you use it instead of investing upfront costs in data centers: 
This process is an example of Trade upfront expense for variable expense.
Scaling your infrastructure capacity in and out to meet demand: This process is an example of Stop guessing capacity.


=> QUIZ

1 - Which pillar of the AWS Well-Architected Framework includes the ability to run workloads effectively and gain insights into their operations?

a) Cost Optimization

b) Operational Excellence

c) Performance Efficiency

d) Reliability

The correct response option is Operational Excellence.


The other response options are incorrect because:

The Cost Optimization pillar focuses on the ability to run systems to deliver business value at the lowest price point.
The Performance Efficiency pillar focuses on using computing resources efficiently to meet system requirements and 
to maintain that efficiency as demand changes and technologies evolve.
The Reliability pillar focuses on the ability of a workload to consistently and correctly perform its intended functions.


2 - What are the benefits of cloud computing? (Select TWO.)

a) Increase speed and agility.

b) Benefit from smaller economies of scale.

c) Trade variable expense for upfront expense.

d) Maintain infrastructure capacity.

e) Stop spending money running and maintaining data centers.

The two correct response options are: 
    Increase speed and agility.
    Stop spending money running and maintaining data centers.


The six advantages of cloud computing are:
    Trade upfront expense for variable expense.
    Benefit from massive economies of scale.
    Stop guessing capacity.
    Increase speed and agility.
    Stop spending money running and maintaining data centers.
    Go global in minutes.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------

MODULE 11
Determine resources for preparing for the AWS Certified Cloud Practitioner exam.
Describe the benefits of becoming AWS Certified.

=> Exam domains
The AWS Certified Cloud Practitioner exam includes four domains:
    Cloud Concepts              26%
    Security and Compliance     25%
    Technology                  33%
    Billing and Pricing         16%


=> Exam details
The AWS Certified Cloud Practitioner exam consists of 65 questions to be completed in 90 minutes. The minimum passing score is 70%.
Two types of questions are included on the exam: multiple choice and multiple response.
    A multiple-choice question has one correct response and three incorrect responses, or distractors.
    A multiple-response question has two or more correct responses out of five or more options.


=> Whitepapers recommended
https://d1.awsstatic.com/whitepapers/aws-overview.pdf
http://d1.awsstatic.com/whitepapers/aws_pricing_overview.pdf
https://aws.amazon.com/premiumsupport/plans/


=> QUIZ

1 - You are running an Amazon EC2 instance and want to store data in an attached resource. 
Your data is temporary and will not be kept long term. Which resource should you use?

a) Amazon S3 bucket

b) Instance store

c) Amazon Elastic Block Store (Amazon EBS) volume

d) Subnet

The correct response option is instance store.

Instance stores are ideal for temporary data that does not need to be kept long term. 
When an Amazon EC2 instance is stopped or terminated, all the data that has been written to the attached instance store is deleted.


The other response options are incorrect because:

Amazon EBS volumes are ideal for data that needs to be retained. When an Amazon EC2 instance is stopped or terminated, 
all of the data on the attached EBS volume is still available.
Amazon S3 buckets cannot be attached to Amazon EC2 instances.
A subnet is a section of a virtual private cloud (VPC) in which you can group resources based on security or operational needs.


2 - Which service enables you to review details for user activities and API calls that have occurred within your AWS environment?

a) Amazon CloudWatch

b) AWS CloudTrail

c) Amazon Inspector

d) AWS Trusted Advisor


The correct response option is AWS CloudTrail.

With CloudTrail, you can view a complete history of user activity and API calls for your applications and resources. 
Events are typically updated in CloudTrail within 15 minutes after an API call was made. 
You can filter events by specifying the time and date that an API call occurred, the user who requested the action, 
the type of resource that was involved in the API call, and more.


The other response options are incorrect because:

Amazon CloudWatch is a service that provides data that you can use to monitor your applications, optimize resource utilization, 
and respond to system-wide performance changes.
Amazon Inspector is a service that checks applications for security vulnerabilities and deviations from security best practices.
AWS Trusted Advisor is an online tool that inspects your AWS environment and provides real-time guidance in accordance with AWS best practices.


3 - Which service enables you to build the workflows that are required for human review of machine learning predictions?

a) Amazon Augmented AI

b) Amazon Textract

c) Amazon Lex

d) Amazon Aurora

The correct response option is Amazon Augmented AI.

Amazon Augmented AI (Amazon A2I) provides built-in human review workflows for common machine learning use cases, 
such as content moderation and text extraction from documents. With Amazon A2I, you can also create your own workflows for machine learning models built on Amazon SageMaker or any other tools.


The other response options are incorrect because:

Amazon Textract is a machine learning service that automatically extracts text and data from scanned documents.
Amazon Lex is a service that enables you to build conversational interfaces using voice and text.
Amazon Aurora is an enterprise-class relational database.


4 - Which pillar of the AWS Well-Architected Framework focuses on using computing resources in ways that meet system requirements?

a) Security

b) Operational Excellence

c) Performance Efficiency

d) Reliability

The correct response option is Performance Efficiency.

The Performance Efficiency pillar focuses on using computing resources efficiently to meet system requirements, 
and to maintain that efficiency as demand changes and technologies evolve.


The other responses are incorrect because:

The Operational Excellence pillar includes the ability to run workloads effectively, gain insights into their operations, 
and continuously improve supporting processes to deliver business value. 
The Security pillar focuses on protecting data, systems, and assets. 
It also focuses on using cloud technologies to improve the security of your workloads.
The Reliability pillar focuses on the ability of a workload to consistently and correctly perform its intended functions.


5 - Which Support plans include access to all AWS Trusted Advisor checks? (Select TWO.)

a) Basic

b) Developer

c) AWS Free Tier

d) Business

e) Enterprise

The two correct response options are:
    Enterprise
    Business
The other response options are incorrect because:
The Basic and Developer Support plans provide access to a limited selection of AWS Trusted Advisor checks.
The AWS Free Tier is not a Support plan. It is a program that consists of three types of offers that allow customers 
to use AWS services without incurring costs: Always free, 12 months free, and Trials.


6 - Which tool is used to automate actions for AWS services and applications through scripts? 

a) AWS Snowball

b) AWS Command Line Interface

c) Amazon QLDB

d) Amazon Redshift

The correct response option is AWS Command Line Interface.

The AWS Command Line Interface (AWS CLI) enables you to control multiple AWS services directly from the command line within one tool.
For example, you can use commands to start an Amazon EC2 instance, connect an Amazon EC2 instance to a specific Auto Scaling group, and more.
The AWS CLI is available for users on Windows, macOS, and Linux.


The other response options are incorrect because:

Amazon Redshift is a data warehousing service that you can use for big data analytics. It offers the ability to collect data 
from many sources and help you to understand relationships and trends across your data. 
Amazon Quantum Ledger Database (Amazon QLDB) is a ledger database service. You can use Amazon QLDB to review a complete history 
of all the changes that have been made to your application data.
AWS Snowball is a device that enables you to transfer large amounts of data into and out of AWS.


7 - Which action can you perform in Amazon CloudFront?

a) Run infrastructure in a hybrid cloud approach.

b) Provision resources by using programming languages or a text file.

c) Deliver content to customers through a global network of edge locations.

d) Provision an isolated section of the AWS Cloud to launch resources in a virtual network that you define.

The correct response is Deliver content to customers through a global network of edge locations.

Amazon CloudFront is a content delivery service. 
It uses a network of edge locations to cache content and deliver content to customers all over the world. When content is cached, 
it is stored locally as a copy. This content might be video files, photos, webpages, and so on.


The other response options are incorrect because:

Run infrastructure in a hybrid cloud approach - This action can be performed with AWS Outposts. 
Provision resources by using programming languages or a text file - This action can be performed in AWS CloudFormation.
Provision an isolated section of the AWS Cloud to launch resources in a virtual network that you define - This action can be performed in 
Amazon Virtual Private Cloud (Amazon VPC).


8 - You want to store data in a key-value database. Which service should you use?

a) Amazon RDS

b) Amazon DocumentDB

c) Amazon Aurora

d) Amazon DynamoDB

The correct response option is Amazon DynamoDB.

Amazon DynamoDB is a key-value database service. A key-value database might include data pairs 
such as “Name: John Doe,” “Address: 123 Any Street,” and “City: Anytown”.

In a key-value database, you can add or remove attributes from items in the table at any time.
Additionally, not every item in the table has to have the same attributes.  


The other response options are incorrect because:

Amazon Relational Database Service (Amazon RDS) and Amazon Aurora use structured query language (SQL) 
to store and query data. They are not key-value databases.
Amazon DocumentDB is a document database service that supports MongoDB workloads.


9 - Which service is used to run containerized applications on AWS?

a) Amazon SageMaker

b) Amazon Elastic Kubernetes Service (Amazon EKS)

c) Amazon Aurora

d) Amazon Redshift

The correct response option is Amazon Elastic Kubernetes Service (Amazon EKS).

Amazon EKS is a fully managed service that you can use to run Kubernetes on AWS. Kubernetes is open-source software 
that enables you to deploy and manage containerized applications at scale.
Containers provide you with a standard way to package your application's code and dependencies into a single object.
Containers are frequently used for processes and workflows in which there are essential requirements for security, reliability, and scalability.


The other response options are incorrect because:

Amazon SageMaker is a service that enables you to quickly build, train, and deploy machine learning models.
Amazon Aurora is an enterprise-class relational database. 
Amazon Redshift is a data warehousing service that you can use for big data analytics.


10 - Which AWS Trusted Advisor category includes checks for your service limits and overutilized instances?

a) Security

b) Cost Optimization

c) Fault Tolerance

d) Performance

The correct response option is Performance.

In this category, AWS Trusted Advisor also helps improve the performance of your services by providing 
recommendations for how to take advantage of provisioned throughput.


The other response options are incorrect because:

The Security category includes checks that help you to review your permissions and identify which AWS security features to enable.
The Cost Optimization category includes checks for unused or idle resources that could be eliminated and provide cost savings.
The Fault Tolerance category includes checks to help you improve your applications’ availability and redundancy.


11 - Which statement best describes AWS Marketplace?

a) A resource that provides guidance, architectural reviews, and ongoing communication with your company as you plan, deploy, and optimize your applications

b) A resource that can answer questions about best practices and assist with troubleshooting issues

c) A digital catalog that includes thousands of software listings from independent software vendors

d) An online tool that inspects your AWS environment and provides real-time guidance in accordance with AWS best practices

The correct response option is A digital catalog that includes thousands of listings from independent software vendors.

You can use AWS Marketplace to find, test, and buy software that runs on AWS.


The other response options are incorrect because:

A resource that can answer questions about best practices and assist with troubleshooting issues - This response option describes AWS Support.
A resource that provides guidance, architectural reviews, and ongoing communication with your company as 
you plan, deploy, and optimize your applications - This response option describes a Technical Account Manager (TAM). 
An online tool that inspects your AWS environment and provides real-time guidance 
in accordance with AWS best practices - This response option describes AWS Trusted Advisor.


12 - Which migration strategy involves changing how an application is architected and developed, typically by using cloud-native features?

a) Rehosting

b) Refactoring

c) Replatforming

d) Repurchasing

The correct response option is Refactoring.


The other response options are incorrect because:

Repurchasing involves replacing an existing application with a cloud-based version, such as software found in AWS Marketplace.
Rehosting involves moving an application to the cloud with little to no modifications 
to the application itself. It is also known as “lift and shift.”
Replatforming involves selectively optimizing aspects of an application to achieve benefits in the cloud 
without changing the core architecture of the application. It is also known as “lift, tinker, and shift.”


13 - Which actions can you perform in Amazon Route 53? (Select TWO.)

a) Access AWS security and compliance reports and select online agreements.

b) Connect user requests to infrastructure in AWS and outside of AWS.

c) Manage DNS records for domain names.

d) Automate the deployment of workloads into your AWS environment.

e) Monitor your applications and respond to system-wide performance changes.

The correct two response options are:

Connect user requests to infrastructure in AWS and outside of AWS.
Manage DNS records for domain names. 
Amazon Route 53 is a DNS web service. It gives developers and businesses a reliable way to route end users to internet applications that are hosted in AWS. 

Additionally, you can transfer DNS records for existing domain names that are currently managed by other domain registrars, 
or register new domain names directly within Amazon Route 53.


The other response options are incorrect because:

Monitor your applications and respond to system-wide performance changes - These actions can be performed in Amazon CloudWatch.
Access AWS security and compliance reports and special online agreements - This action can be performed in AWS Artifact.
Automate the deployment of workloads into your AWS environment - This action can be performed with AWS Quick Starts.


14 - Which tasks are the responsibilities of AWS? (Select TWO.)

a) Maintaining virtualization infrastructure

b) Configuring security groups on Amazon EC2 instances

c) Creating IAM users and groups

d) Configuring AWS infrastructure devices

e) Training company employees on how to use AWS services

The two correct response options are:

Maintaining virtualization infrastructure
Configuring AWS infrastructure devices 
The other three response options are tasks that are the responsibilities of customers.


15 - Which compute option reduces costs when you commit to a consistent amount of compute usage for a 1-year or 3-year term?

a) Spot Instances

b) Reserved Instances

c) Savings Plans

d) Dedicated Hosts

The correct response option is Savings Plans.

Amazon EC2 Savings Plans enable you to reduce your compute costs by committing to a consistent amount of compute usage 
for a 1-year or 3-year term. This results in savings of up to 72% over On-Demand Instance costs.
Any usage up to the commitment is charged at the discounted Savings Plan rate (for example, $10 an hour).
Any usage beyond the commitment is charged at regular On-Demand Instance rates.


The other response options are incorrect because:

Reserved Instances are a billing discount that is applied to the use of On-Demand Instances in your account.
You can purchase Standard Reserved and Convertible Reserved Instances for a one-year or three-year term, and 
Scheduled Reserved Instances for a one-year term. Unlike Savings Plans, Reserved Instances do not require you to commit 
to a consistent amount of compute usage over the duration of the contract.

Spot Instances are ideal for workloads with flexible start and end times or that can withstand interruptions. 
Spot Instances leverage unused EC2 computing capacity and offer you cost savings at up to 90% of On-Demand Instance prices.

Dedicated Hosts are physical servers with EC2 instance capacity that is fully dedicated to your use. 
You can use your existing per-socket, per-core, or per-VM software licenses to help maintain license compliance. 
You can purchase On-Demand Dedicated Hosts or Reserved Dedicated Hosts. Of all the Amazon EC2 options that were covered in this course, 
Dedicated Hosts are the most expensive.


16 - You want to send and receive messages between distributed application components. Which service should you use?    

a) Amazon ElastiCache

b) Amazon Route 53

c) Amazon Simple Queue Service (Amazon SQS)

d) AWS Snowball

The correct response option is Amazon Simple Queue Service (Amazon SQS).

Amazon SQS is a message queuing service. Using Amazon SQS, you can send, store, and receive messages between software components at any volume size, 
without losing messages or requiring other services to be available. 
In Amazon SQS, an application sends messages into a queue. A user or service retrieves a message 
from the queue, processes it, and then deletes it from the queue.


The other response options are incorrect because: 

AWS Snowball is a device that enables you to transfer large amounts of data into and out of AWS.
Amazon ElastiCache is a service that adds caching layers on top of your databases to help improve the read times of common requests.

Amazon Route 53 is a DNS web service. It gives developers and businesses a reliable way to route end users to 
internet applications that are hosted in AWS. Additionally, you can transfer DNS records for existing domain names 
that are currently managed by other domain registrars or register new domain names directly in Amazon Route 53.


17 - You want to store data in a volume that is attached to an Amazon EC2 instance. Which service should you use?

a) Amazon ElastiCache

b) AWS Lambda

c) Amazon Elastic Block Store (Amazon EBS)

d) Amazon Simple Storage Service (Amazon S3)

The correct response option is Amazon Elastic Block Store (Amazon EBS).

Amazon EBS provides block-level storage volumes that you can use with Amazon EC2 instances.
If you stop or terminate an Amazon EC2 instance, all the data on the attached EBS volume remains available.


The other response options are incorrect because:

Amazon Simple Storage Service (Amazon S3) is a service that provides object-level storage. Amazon S3 stores data as objects within buckets.
AWS Lambda is a service that lets you run code without provisioning or managing servers.
Amazon ElastiCache is a service that adds caching layers on top of your databases to help improve the read times of common requests.


18 - Which component or service enables you to establish a dedicated private connection between your data center and virtual private cloud (VPC)?

a) Internet gateway

b) AWS Direct Connect

c) Virtual private gateway

d) Amazon CloudFront

The correct response option is AWS Direct Connect.

AWS Direct Connect is a service that enables you to establish a dedicated private connection between your data center and VPC. 
The private connection that AWS Direct Connect provides helps you to reduce network costs and 
increase the amount of bandwidth that can travel through your network.


The other response options are incorrect because:

Amazon CloudFront is a content delivery service. It uses a network of edge locations 
to cache content and deliver content to customers all over the world.

A virtual private gateway enables you to establish a virtual private network (VPN) connection between your VPC and a private network, 
such as an on-premises data center or internal corporate network. 
A virtual private gateway allows traffic into the VPC only if it is coming from an approved network.

An internet gateway is a connection between a VPC and the internet. It allows public traffic from the internet to access a VPC.


19 - Which statement best describes an Availability Zone?

a) A site that Amazon CloudFront uses to cache copies of content for faster delivery to users at any location

b) A separate geographical location with multiple locations that are isolated from each other

c) A fully isolated portion of the AWS global infrastructure

d) The server from which Amazon CloudFront gets your files

The correct response option is A fully isolated portion of the AWS global infrastructure.

An Availability Zone is a single data center or a group of data centers within a Region. 
Availability Zones are located tens of miles apart from each other. This helps them to provide interconnectivity 
to support the services and applications that run within a Region.


The other response options are incorrect because:

A separate geographical location with multiple locations that are isolated from each other - This response option describes a Region.
The server from which Amazon CloudFront gets your files - This response option describes an origin.
A site that Amazon CloudFront uses to cache copies of content for faster delivery 
to users at any location - This response option describes an Edge location.


20 - Which statement best describes Elastic Load Balancing?

a) A service that distributes incoming traffic across multiple targets, such as Amazon EC2 instances

b) A service that monitors your applications and automatically adds or removes capacity from your resource groups in response to changing demand

c) A service that enables you to set up, manage, and scale a distributed in-memory or cache environment in the cloud

d) A service that provides data that you can use to monitor your applications, optimize resource utilization, and respond to system-wide performance changes

The correct response option is A service that distributes incoming traffic across multiple targets, such as Amazon EC2 instances.

A load balancer acts as a single point of contact for all incoming web traffic to your Auto Scaling group. 
This means that as Amazon EC2 instances are added or removed in response to the amount of incoming traffic, 
these requests are routed to the load balancer first and then spread across multiple resources that will handle them.


The other response options are incorrect because:

A service that monitors your applications and automatically adds or removes capacity from your 
resource groups in response to changing demand - This response option describes AWS Auto Scaling.

A service that provides data that you can use to monitor your applications, optimize resource utilization, 
and respond to system-wide performance changes - This response option describes Amazon CloudWatch. 
Although Elastic Load Balancing does optimize resource utilization by distributing incoming traffic across available resources, 
this would not be the best response option because Elastic Load Balancing does not provide all the other listed features.

A service that enables you to set up, manage, and scale a distributed in-memory or 
cache environment in the cloud - This response option describes Amazon ElastiCache.


21 - Which virtual private cloud (VPC) component controls inbound and outbound traffic for Amazon EC2 instances?

a) Security group

b) Internet gateway

c) Subnet

d) Network access control list

The correct response option is security group.

A security group is a virtual firewall that controls inbound and outbound traffic for an Amazon EC2 instance. 
By default, a security group denies all inbound traffic and allows all outbound traffic. You can add custom 
rules to configure which traffic should be allowed or denied.


The other response options are incorrect because:

A subnet is a section of a VPC in which you can group resources based on security or operational needs.
A network access control list (ACL) is a virtual firewall that controls inbound and outbound traffic at the subnet level.
An internet gateway is a connection between a VPC and the internet. It allows public traffic from the internet to access a VPC.


22 - Which statement is TRUE for AWS Lambda?

a) Before using AWS Lambda, you must prepay for your estimated compute time.

b) The first step in using AWS Lambda is provisioning a server.

c) To use AWS Lambda, you must configure the servers that run your code.

d) You pay only for compute time while your code is running.

The correct response option is You pay only for compute time while your code is running.

AWS Lambda is a service that lets you run code without needing to provision or manage servers.
While using AWS Lambda, you pay only for the compute time that you consume. 
You are charged only when your code is running. With AWS Lambda, you can run code for virtually any 
type of application or backend service, all with zero administration.


23 - You want Amazon S3 to monitor your objects’ access patterns. Which storage class should you use? 

a) S3 One Zone-IA

b) S3 Intelligent-Tiering

c) S3 Glacier

d) S3 Standard-IA

The correct response option is S3 Intelligent-Tiering.

In the S3 Intelligent-Tiering storage class, Amazon S3 monitors objects’ access patterns.
If you haven’t accessed an object for 30 consecutive days, Amazon S3 automatically moves it to the infrequent access tier, 
S3 Standard-IA. If you access an object in the infrequent access tier, Amazon S3 automatically moves it to the frequent access tier, S3 Standard.


The other response options are incorrect because:

S3 Glacier is a low-cost storage class that is ideal for data archiving. You can retrieve objects stored in the S3 Glacier storage 
class within a few minutes to a few hours.

The S3 Standard-IA storage class is ideal for data that is infrequently accessed but requires high availability when needed. 
Both S3 Standard and S3 Standard-IA store data in a minimum of three Availability Zones. S3 Standard-IA provides the same level 
of availability as S3 Standard but at a lower storage price.

S3 One Zone-IA is ideal for infrequently accessed data that does not require high availability.


24 - Which tool enables you to visualize, understand, and manage your AWS costs and usage over time?

a) AWS Pricing Calculator

b) AWS Artifact

c) AWS Cost Explorer

d) AWS Budgets

The correct response option is AWS Cost Explorer.

With AWS Cost Explorer, you can quickly create custom reports to analyze your AWS cost and usage data.


The other response options are incorrect because:

AWS Budgets lets you set custom alerts that will notify you when your service usage exceeds 
(or is forecasted to exceed) the amount that you have budgeted.

AWS Pricing Calculator lets you explore AWS services and create an estimate for the cost of your use cases on AWS.
In the AWS Pricing Calculator, you can enter details for your cloud computing requirements and 
then receive a detailed estimate that can be exported and shared.

AWS Artifact is a service that enables you to access AWS security and compliance reports and special online agreements.


25 - Which service is used to quickly deploy and scale applications on AWS?

a) AWS Outposts

b) AWS Elastic Beanstalk

c) AWS Snowball

d) Amazon CloudFront

The correct response option is AWS Elastic Beanstalk.

You upload your application, and Elastic Beanstalk automatically handles the deployment details of capacity provisioning, 
load balancing, auto-scaling, and application health monitoring.


The other response options are incorrect because:

AWS Outposts is a service that enables you to run infrastructure in a hybrid cloud approach.
Amazon CloudFront is a content delivery service. 
AWS Snowball is a device that enables you to transfer large amounts of data into and out of AWS.


26 - In the S3 Intelligent-Tiering storage class, Amazon S3 moves objects between a frequent access tier and 
an infrequent access tier. Which storage classes are used for these tiers? (Select TWO.)

a) S3 One Zone-IA

b) S3 Glacier

c) S3 Standard

d) S3 Standard-IA

e) S3 Glacier Deep Archive

The two correct response options are:

S3 Standard
S3 Standard-IA 
In the S3 Intelligent-Tiering storage class, Amazon S3 monitors objects’ access patterns.If you haven’t accessed an object 
for 30 consecutive days, Amazon S3 automatically moves it to the infrequent access tier, S3 Standard-IA. 
If you access an object in the infrequent access tier, Amazon S3 automatically moves it to the frequent access tier, S3 Standard.


27 - Which service enables you to consolidate and manage multiple AWS accounts from a central location?

a) AWS Organizations

b) AWS Artifact

c) AWS Identity and Access Management (IAM)

d) AWS Key Management Service (AWS KMS)

The correct response option is AWS Organizations.

In AWS Organizations, you can centrally control permissions for the accounts in your organization by using service control policies (SCPs).
Additionally, you can use the consolidated billing feature in AWS Organizations to combine usage and receive a single bill for multiple AWS accounts.


The other response options are incorrect because:

AWS Identity and Access Management (IAM) is a service that you can use to manage access to AWS services and resources.  
AWS Artifact is a service that enables you to access AWS security and compliance reports and special online agreements.
AWS Key Management Service (AWS KMS) enables you to create, manage, and use cryptographic keys.


28 - Which statement best describes Amazon GuardDuty?

a) A service that checks applications for security vulnerabilities and deviations from security best practices

b) A service that provides intelligent threat detection for your AWS infrastructure and resources

c) A service that lets you monitor network requests that come into your web applications

d) A service that helps protect your applications against distributed denial-of-service (DDoS) attacks

The correct response option is A service that provides intelligent threat detection for your AWS infrastructure and resources.

AWS GuardDuty identifies threats by continually monitoring the network activity and account behavior within your AWS environment.


The other response options are incorrect because:

A service that helps protect your applications against distributed denial-of-service (DDoS) attacks - This response option describes AWS Shield.
A service that checks applications for security vulnerabilities and deviations from security best practices - This response option describes Amazon Inspector.
A service that lets you monitor network requests that come into your web applications - This response option describes AWS WAF.


29 - Which service is used to transfer up to 100 PB of data to AWS?

a) Amazon CloudFront

b) Amazon Neptune

c) AWS DeepRacer

d) AWS Snowmobile

The correct response option is AWS Snowmobile.

AWS Snowmobile is a service that is used for transferring up to 100 PB of data to AWS. Each Snowmobile is a 45-foot long shipping 
container that is pulled by a semi-trailer truck. 


The other response options are incorrect because:

Amazon Neptune is a graph database service. You can use Amazon Neptune to build and run applications that work with highly 
connected datasets, such as recommendation engines, fraud detection, and knowledge graphs.
Amazon CloudFront is a content delivery service.
AWS DeepRacer is an autonomous 1/18 scale race car that you can use to test reinforcement learning models.


30 - Which Perspective of the AWS Cloud Adoption Framework focuses on recovering IT workloads to meet the requirements of your business stakeholders?

a) People Perspective

b) Operations Perspective

c) Governance Perspective

d) Business Perspective

The correct response option is Operations Perspective.

The Operations Perspective of the AWS Cloud Adoption Framework also includes principles for operating in the cloud by using agile best practices.


The other response options are incorrect because: 

The Business Perspective helps you to move from a model that separates business and IT strategies into a business model that integrates IT strategy.
The People Perspective helps Human Resources (HR) employees prepare their teams for cloud adoption by updating organizational 
processes and staff skills to include cloud-based competencies.
The Governance Perspective helps you understand how to update the staff skills and organizational processes that are necessary to ensure business governance in the cloud.

